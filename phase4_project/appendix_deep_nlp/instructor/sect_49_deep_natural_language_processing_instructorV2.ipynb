{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 43: Foundations of Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bonus Office Hours S.G.\n",
    "- 10/27/20\n",
    "- online-ds-ft-070620"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You are not allowed to use any propriety functions from this notebook in your projects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss word Embeddings and their advantages\n",
    "- Training Word2Vec models\n",
    "- Using pretrained word embeddings\n",
    "\n",
    "\n",
    "- Create a Classification Model for true-trump (\"Twitter for Android\") vs trump-staffer(\"Twitter for iPhone - from period of time when android was still in use)\n",
    "\n",
    "    - Use lesson's W2Vec class in Sci-kit learn models\n",
    "    - Use LSTMs\n",
    "    - Use RNN/GRUs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Compare:\n",
    "    1.  Mean embeddings vs count/tfidf data with scikit learn.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My Work-in-Progress Capstone v2.0 Notebook:\n",
    "    - [GitHub Notebook Link](https://github.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/blob/WIP/Capstone%20Restarted%2010-2020.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP & Word Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **_Natural Language Processing_**, or **_NLP_**, is the study of how computers can interact with humans through the use of human language.  Although this is a field that is quite important to Data Scientists, it does not belong to Data Science alone.  NLP has been around for quite a while, and sits at the intersection of *Computer Science*, *Artificial Intelligence*, *Linguistics*, and *Information Theory*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/embeddings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert words into a vector space\n",
    "    + Mathematical object\n",
    "- It's all about closeness\n",
    "    + Distributional Hypothesis: https://en.wikipedia.org/wiki/Distributional_semantics#Distributional_hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-word-embeddings-online-ds-ft-100719/master/images/vectors.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaggle Tutorial:  https://www.kaggle.com/learn/embeddings\n",
    "- Google Embedding Crash Course: https://developers.google.com/machine-learning/crash-course/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/training_data.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train the MLP to find the best weights (context) to map word-to-word\n",
    "- But since words close to another usually contain context, we're _really_ teaching it context in those weights\n",
    "- Gut check: similar contexted words can be exchanged\n",
    "    + EX: \"A fluffy **dog** is a great pet\" <--> \"A fluffy **cat** is a great pet\"\n",
    "\n",
    "- By training a text-generation model, we wind up with a lookup table where each word has its own vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-using-word2vec-online-ds-ft-100719/master/images/new_skip_gram_net_arch.png\">\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/learn-co-students/dsc-using-word2vec-online-ds-ft-100719/master/images/new_word2vec_weight_matrix_lookup_table.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word will have a vector of contexts: the embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Word Embeddings with Trump's Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:17:22.516879Z",
     "start_time": "2020-10-27T16:17:15.774406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fsds v0.2.27 loaded.  Read the docs: https://fs-ds.readthedocs.io/en/latest/ \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "</style><table id=\"T_e4229080_186f_11eb_a3f8_acde48001122\" ><caption>Loaded Packages and Handles</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >Handle</th>        <th class=\"col_heading level0 col1\" >Package</th>        <th class=\"col_heading level0 col2\" >Description</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row0_col0\" class=\"data row0 col0\" >dp</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row0_col1\" class=\"data row0 col1\" >IPython.display</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row0_col2\" class=\"data row0 col2\" >Display modules with helpful display and clearing commands.</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row1_col0\" class=\"data row1 col0\" >fs</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row1_col1\" class=\"data row1 col1\" >fsds</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row1_col2\" class=\"data row1 col2\" >Custom data science bootcamp student package</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row2_col0\" class=\"data row2 col0\" >mpl</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row2_col1\" class=\"data row2 col1\" >matplotlib</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row2_col2\" class=\"data row2 col2\" >Matplotlib's base OOP module with formatting artists</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row3_col0\" class=\"data row3 col0\" >plt</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row3_col1\" class=\"data row3 col1\" >matplotlib.pyplot</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row3_col2\" class=\"data row3 col2\" >Matplotlib's matlab-like plotting module</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row4_col0\" class=\"data row4 col0\" >np</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row4_col1\" class=\"data row4 col1\" >numpy</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row4_col2\" class=\"data row4 col2\" >scientific computing with Python</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row5_col0\" class=\"data row5 col0\" >pd</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row5_col1\" class=\"data row5 col1\" >pandas</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row5_col2\" class=\"data row5 col2\" >High performance data structures and tools</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row6_col0\" class=\"data row6 col0\" >sns</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row6_col1\" class=\"data row6 col1\" >seaborn</td>\n",
       "                        <td id=\"T_e4229080_186f_11eb_a3f8_acde48001122row6_col2\" class=\"data row6 col2\" >High-level data visualization library based on matplotlib</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7ffe3643f6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[i] Pandas .iplot() method activated.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U fsds\n",
    "from fsds.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:17:24.456839Z",
     "start_time": "2020-10-27T16:17:22.519840Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>is_retweet</th>\n",
       "      <th>id_str</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:37:57</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>My thoughts and prayers are with those affecte...</td>\n",
       "      <td>12-01-2016 14:37:57</td>\n",
       "      <td>12077</td>\n",
       "      <td>65724</td>\n",
       "      <td>False</td>\n",
       "      <td>804333718999539712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 14:38:09</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>Getting ready to leave for the Great State of ...</td>\n",
       "      <td>12-01-2016 14:38:09</td>\n",
       "      <td>9834</td>\n",
       "      <td>57249</td>\n",
       "      <td>False</td>\n",
       "      <td>804333771021570048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-01 22:52:10</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Heading to U.S. Bank Arena in Cincinnati Ohio ...</td>\n",
       "      <td>12-01-2016 22:52:10</td>\n",
       "      <td>5564</td>\n",
       "      <td>31256</td>\n",
       "      <td>False</td>\n",
       "      <td>804458095569158144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-02 02:45:18</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Thank you Ohio! Together we made history ‚Äì and...</td>\n",
       "      <td>12-02-2016 02:45:18</td>\n",
       "      <td>17283</td>\n",
       "      <td>72196</td>\n",
       "      <td>False</td>\n",
       "      <td>804516764562374656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-03 00:44:20</th>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>The President of Taiwan CALLED ME today to wis...</td>\n",
       "      <td>12-03-2016 00:44:20</td>\n",
       "      <td>24700</td>\n",
       "      <td>111106</td>\n",
       "      <td>False</td>\n",
       "      <td>804848711599882240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:17:43</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @SenJohnKennedy: I think Speaker Pelosi is ...</td>\n",
       "      <td>01-01-2020 01:17:43</td>\n",
       "      <td>8893</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181071988703232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:18:47</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>RT @DanScavino: https://t.co/CJRPySkF1Z</td>\n",
       "      <td>01-01-2020 01:18:47</td>\n",
       "      <td>10796</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1212181341078458369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:22:28</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Our fantastic First Lady! https://t.co/6iswto4WDI</td>\n",
       "      <td>01-01-2020 01:22:28</td>\n",
       "      <td>27567</td>\n",
       "      <td>132633</td>\n",
       "      <td>False</td>\n",
       "      <td>1212182267113680896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 01:30:35</th>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>HAPPY NEW YEAR!</td>\n",
       "      <td>01-01-2020 01:30:35</td>\n",
       "      <td>85409</td>\n",
       "      <td>576045</td>\n",
       "      <td>False</td>\n",
       "      <td>1212184310389850119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 03:12:07</th>\n",
       "      <td>Twitter Media Studio</td>\n",
       "      <td>https://t.co/EVAEYD1AgV</td>\n",
       "      <td>01-01-2020 03:12:07</td>\n",
       "      <td>25016</td>\n",
       "      <td>108830</td>\n",
       "      <td>False</td>\n",
       "      <td>1212209862094012416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14066 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   source  \\\n",
       "datetime                                    \n",
       "2016-12-01 14:37:57    Twitter for iPhone   \n",
       "2016-12-01 14:38:09   Twitter for Android   \n",
       "2016-12-01 22:52:10    Twitter for iPhone   \n",
       "2016-12-02 02:45:18    Twitter for iPhone   \n",
       "2016-12-03 00:44:20   Twitter for Android   \n",
       "...                                   ...   \n",
       "2020-01-01 01:17:43    Twitter for iPhone   \n",
       "2020-01-01 01:18:47    Twitter for iPhone   \n",
       "2020-01-01 01:22:28    Twitter for iPhone   \n",
       "2020-01-01 01:30:35    Twitter for iPhone   \n",
       "2020-01-01 03:12:07  Twitter Media Studio   \n",
       "\n",
       "                                                                  text  \\\n",
       "datetime                                                                 \n",
       "2016-12-01 14:37:57  My thoughts and prayers are with those affecte...   \n",
       "2016-12-01 14:38:09  Getting ready to leave for the Great State of ...   \n",
       "2016-12-01 22:52:10  Heading to U.S. Bank Arena in Cincinnati Ohio ...   \n",
       "2016-12-02 02:45:18  Thank you Ohio! Together we made history ‚Äì and...   \n",
       "2016-12-03 00:44:20  The President of Taiwan CALLED ME today to wis...   \n",
       "...                                                                ...   \n",
       "2020-01-01 01:17:43  RT @SenJohnKennedy: I think Speaker Pelosi is ...   \n",
       "2020-01-01 01:18:47            RT @DanScavino: https://t.co/CJRPySkF1Z   \n",
       "2020-01-01 01:22:28  Our fantastic First Lady! https://t.co/6iswto4WDI   \n",
       "2020-01-01 01:30:35                                    HAPPY NEW YEAR!   \n",
       "2020-01-01 03:12:07                            https://t.co/EVAEYD1AgV   \n",
       "\n",
       "                              created_at  retweet_count  favorite_count  \\\n",
       "datetime                                                                  \n",
       "2016-12-01 14:37:57  12-01-2016 14:37:57          12077           65724   \n",
       "2016-12-01 14:38:09  12-01-2016 14:38:09           9834           57249   \n",
       "2016-12-01 22:52:10  12-01-2016 22:52:10           5564           31256   \n",
       "2016-12-02 02:45:18  12-02-2016 02:45:18          17283           72196   \n",
       "2016-12-03 00:44:20  12-03-2016 00:44:20          24700          111106   \n",
       "...                                  ...            ...             ...   \n",
       "2020-01-01 01:17:43  01-01-2020 01:17:43           8893               0   \n",
       "2020-01-01 01:18:47  01-01-2020 01:18:47          10796               0   \n",
       "2020-01-01 01:22:28  01-01-2020 01:22:28          27567          132633   \n",
       "2020-01-01 01:30:35  01-01-2020 01:30:35          85409          576045   \n",
       "2020-01-01 03:12:07  01-01-2020 03:12:07          25016          108830   \n",
       "\n",
       "                    is_retweet               id_str  \n",
       "datetime                                             \n",
       "2016-12-01 14:37:57      False   804333718999539712  \n",
       "2016-12-01 14:38:09      False   804333771021570048  \n",
       "2016-12-01 22:52:10      False   804458095569158144  \n",
       "2016-12-02 02:45:18      False   804516764562374656  \n",
       "2016-12-03 00:44:20      False   804848711599882240  \n",
       "...                        ...                  ...  \n",
       "2020-01-01 01:17:43       True  1212181071988703232  \n",
       "2020-01-01 01:18:47       True  1212181341078458369  \n",
       "2020-01-01 01:22:28      False  1212182267113680896  \n",
       "2020-01-01 01:30:35      False  1212184310389850119  \n",
       "2020-01-01 03:12:07      False  1212209862094012416  \n",
       "\n",
       "[14066 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../trump_tweets_12012016_to_01012020.csv\")##'https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')\n",
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "- Two Part Word2Vec Tutorial  (linked from Learn)\n",
    "    - [Part 1: The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "    - [Part 2: Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `sentences`: dataset to train on\n",
    "- `size`: how big of a word vector do we want\n",
    "- `window`: how many words around the target word to train with\n",
    "- `min_count`: how many times the word shows up in corpus; we don't want words that are rarely used\n",
    "- `workers`: number of threads (individual task \"workers\")\n",
    "\n",
    "```python\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Let's assume we have our text corpus already tokenized and stored inside the variable 'data'--the regular text preprocessing steps still need to be handled before training a Word2Vec model!\n",
    "\n",
    "model = Word2Vec(data, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(data, total_examples=model.corpus_count)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:18:47.574878Z",
     "start_time": "2020-10-27T16:18:47.568779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading to U.S. Bank Arena in Cincinnati Ohio for a 7pm rally. Join me! Tickets: https://t.co/HiWqZvHv6M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Heading to U.S. Bank Arena in Cincinnati Ohio for a 7pm rally. Join me! Tickets:  '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "test_str = df['text'].iloc[2]\n",
    "print(test_str)\n",
    "re.sub(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",' ',test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:18:48.050564Z",
     "start_time": "2020-10-27T16:18:48.013627Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Heading to U.S. Bank Arena in Cincinnati Ohio for a 7pm rally. Join me! Tickets:  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "def sub_urls(string,replace_with=' '):\n",
    "    return re.sub(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",replace_with,string)\n",
    "df['text_clean'] = df['text'].apply(lambda x: sub_urls(x,' '))\n",
    "df['text_clean'].iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:18:48.942276Z",
     "start_time": "2020-10-27T16:18:48.889801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Boeing is building a brand new 747 Air Force One for future presidents but costs are out of control more than $4 billion. Cancel order!',\n",
       "       'We cannot continue to let Israel be treated with such total disdain and disrespect. They used to have a great friend in the U.S. but.......',\n",
       "       'RT @DanScavino: On behalf of our next #POTUS &amp; @TeamTrump-#HappyNewYear AMERICAüá∫üá∏https://t.co/Y6XDdijXeahttps://t.co/D8plL7xHqlhttps:‚Ä¶',\n",
       "       '@CNN just released a book called \"Unprecedented\" which explores the 2016 race &amp; victory. Hope it does well but used worst cover photo of me!',\n",
       "       'A beautiful funeral today for a real NYC hero Detective Steven McDonald. Our law enforcement community has my complete and total support.',\n",
       "       'RT @MoskowitzEva: .@BetsyDeVos has the talent commitment and leadership capacity to revitalize our public schools and deliver the promise‚Ä¶',\n",
       "       'Somebody with aptitude and conviction should buy the FAKE NEWS and failing @nytimes and either run it correctly or let it fold with dignity!',\n",
       "       \"Why aren't the lawyers looking at and using the Federal Court decision in Boston which is at conflict with ridiculous lift ban decision?\",\n",
       "       'Our legal system is broken! \"77% of refugees allowed into U.S. since travel reprieve hail from seven suspect countries.\" (WT)  SO DANGEROUS!',\n",
       "       'find the leakers within the FBI itself. Classified information is being given to media that could have a devastating effect on U.S. FIND NOW',\n",
       "       'LinkedIn Workforce Report: January and February were the strongest consecutive months for hiring since August and September 2015',\n",
       "       'If the people of our great country could only see how viciously and inaccurately my administration is covered by certain media!',\n",
       "       'I have great confidence that China will properly deal with North Korea. If they are unable to do so the U.S. with its allies will! U.S.A.',\n",
       "       'RT @foxandfriends: Chicago approves new plan to hide illegal immigrants from the feds plus give them access to city services https://t.co/‚Ä¶',\n",
       "       \"You can't compare anything to ObamaCare because ObamaCare is dead. Dems want billions to go to Insurance Companies to bail out donors....New\",\n",
       "       'President Andrew Jackson who died 16 years before the Civil War started saw it coming and was angry. Would never have let it happen!',\n",
       "       'Melania and I offer our deepest condolences to the family of Otto Warmbier. Full statement: https://t.co/8kmcA6YtFD https://t.co/EhrP4BiJeB',\n",
       "       'Mexico was just ranked the second deadliest country in the world after only Syria. Drug trade is largely the cause. We will BUILD THE WALL!',\n",
       "       'When it comes to the future of America‚Äôs energy needs we will FIND IT we will DREAM IT and we will BUILD IT.‚Ä¶ https://t.co/m0HmPbUZ6C',\n",
       "       'Marine Plane crash in Mississippi is heartbreaking. Melania and I send our deepest condolences to all!',\n",
       "       'After all of these years of suffering thru ObamaCare Republican Senators must come through as they have promised!',\n",
       "       'Great conversations with President @EmmanuelMacron and his representatives on trade military and security.',\n",
       "       'Today it was my privilege to welcome survivors of the #USSArizona to the WH. Remarks: https://t.co/gySxxqazBlhttps://t.co/gySxxqazBl https://t.co/ZJaLAfBcv0',\n",
       "       'As the phony Russian Witch Hunt continues two groups are laughing at this excuse for a lost election taking hold Democrats and Russians!',\n",
       "       'RT @realDonaldTrump: As the phony Russian Witch Hunt continues two groups are laughing at this excuse for a lost election taking hold Dem‚Ä¶',\n",
       "       'Jared Kushner did very well yesterday in proving he did not collude with the Russians. Witch Hunt. Next up 11 year old Barron Trump!',\n",
       "       'After seven years of \"talking\" Repeal &amp; Replace the people of our great country are still being forced to live with imploding ObamaCare!',\n",
       "       'Stock Market could hit all-time high (again) 22000 today. Was 18000 only 6 months ago on Election Day. Mainstream media seldom mentions!',\n",
       "       'RT @dcexaminer: Emails show Washington Post New York Times reporters unenthusiastic about covering Clinton-Lynch meeting https://t.co/PKSL‚Ä¶',\n",
       "       'Just completed call with President Moon of South Korea. Very happy and impressed with 15-0 United Nations vote on North Korea sanctions.',\n",
       "       \"Deepest condolences to the families &amp; fellow officers of the VA State Police who died today. You're all among the best this nation produces.\",\n",
       "       'Sad to see the history and culture of our great country being ripped apart with the removal of our beautiful statues and monuments. You.....',\n",
       "       'Our great country has been divided for decades. Sometimes you need protest in order to heel &amp; we will heel &amp; be stronger than ever before!',\n",
       "       'Our great country has been divided for decades. Sometimes you need protest in order to heal &amp; we will heal &amp; be stronger than ever before!',\n",
       "       \"Last night in Phoenix I read the things from my statements on Charlottesville that the Fake News Media didn't cover fairly. People got it!\",\n",
       "       'Great coordination between agencies at all levels of government. Continuing rains and flash floods are being dealt with. Thousands rescued.',\n",
       "       'RT @FoxNews: .@KellyannePolls on Harvey recovery: We hope when it comes to basic Hurricane Harvey funding that we can rely upon a nonpartis‚Ä¶',\n",
       "       'RT @Franklin_Graham: Join me in praying for @POTUS. He reminded the world ‚ÄúIf the righteous many do not confront the wicked few then evil‚Ä¶',\n",
       "       'Rand Paul is a friend of mine but he is such a negative force when it comes to fixing healthcare. Graham-Cassidy Bill is GREAT! Ends Ocare!',\n",
       "       'RT @GOPChairwoman: The Trump Inaugural Committee is donating $3 million in surplus funds to victims of the latest hurricanes. https://t.co‚Ä¶',\n",
       "       \"RT @Scavino45: Under POTUS' @realDonaldTrump ‚òëÔ∏èS&amp;P 500 38thüìàRecord High ‚òëÔ∏èNASDAQ 44thüìàRecord High‚òëÔ∏è#MakeAmericaGreatAgainüá∫üá∏ https://t.co‚Ä¶\",\n",
       "       'RT @DeptofDefense: VIDEO: Elements of the #DoD and @FEMA are providing humanitarian relief for #PuertoRico üáµüá∑  and #USVI üáªüáÆ . https://t.co/‚Ä¶',\n",
       "       'My warmest condolences and sympathies to the victims and families of the terrible Las Vegas shooting. God bless you!',\n",
       "       'RT @ABCNewsRadio: Global fund championed by Ivanka Trump to help women entrepreneurs begins operations https://t.co/jUmsY3ON4x https://t.co‚Ä¶',\n",
       "       \"Art Laffer just said that he doesn't know how a Democrat could vote against the big tax cut/reform bill and live with themselves!  @FoxNews\",\n",
       "       'The NFL has decided that it will not force players to stand for the playing of our National Anthem. Total disrespect for our great country!',\n",
       "       \"Very little reporting about the GREAT GDP numbers announced yesterday (3.0 despite the big hurricane hits). Best consecutive Q's in years!\",\n",
       "       'After strict consultation with General Kelly the CIA and other Agencies I will be releasing ALL #JFKFiles other than the names and...',\n",
       "       'I hope people will start to focus on our Massive Tax Cuts for Business (jobs) and the Middle Class (in addition to Democrat corruption)!',\n",
       "       'I have great confidence in King Salman and the Crown Prince of Saudi Arabia they know exactly what they are doing....',\n",
       "       'Stock market hit yet another all-time record high yesterday. There is great confidence in the moves that my Administration....',\n",
       "       'Our great country is respected again in Asia. You will see the fruits of our long but successful trip for many years to come!',\n",
       "       'It is actually hard to believe how naive (or dumb) the Failing @nytimes is when it comes to foreign policy...weak and ineffective!',\n",
       "       'Need all on the UN Security Council to vote to renew the Joint Investigative Mechanism for Syria to ensure that Assad Regime does not commit mass murder with chemical weapons ever again.',\n",
       "       'Can you believe that the disrespect for our Country our Flag our Anthem continues without penalty to the players. The Commissioner has lost control of the hemorrhaging league. Players are the boss! https://t.co/udXP5MR8BC',\n",
       "       'We should have a contest as to which of the Networks plus CNN and not including Fox is the most dishonest corrupt and/or distorted in its political coverage of your favorite President (me). They are all bad. Winner to receive the FAKE NEWS TROPHY!',\n",
       "       'RT @FLOTUS: The decorations are up! @WhiteHouse is ready to celebrate! Wishing you a Merry Christmas &amp; joyous holiday season! https://t.co/‚Ä¶',\n",
       "       '‚ÄúThe Conference Board said that consumer sentiment was at its highest level in nearly 17 years in November. The Consumer Confidence Index rose from 126.2 in October to 129.5 notching its best reading since December 2000...‚Äù https://t.co/iNOtT3K8Vn',\n",
       "       'To each member of the graduating class from the National Academy at Quantico CONGRATULATIONS! https://t.co/bGT8S33ZLU',\n",
       "       'Today it was my tremendous honor to visit Marine Helicopter Squadron One (HMX-1) at the Marine Corps Air Facility in Quantico Virginia. I am honored to serve as your Commander-in-Chief. On behalf of an entire Nation THANK YOU for your sacrifice and service. We love you! https://t.co/eLOTc7do52',\n",
       "       'In the East it could be the COLDEST New Year‚Äôs Eve on record. Perhaps we could use a little bit of that good old Global Warming that our Country but not other countries was going to pay TRILLIONS OF DOLLARS to protect against. Bundle up!',\n",
       "       \"RT @charliekirk11: 3 big wins in 2017 you won't hear:Trump confirmed the most circuit court judges ever in a President's 1st year (all co‚Ä¶\",\n",
       "       'On Taxes: ‚ÄúThis is the biggest corporate rate cut ever going back to the corporate income tax rate of roughly 80 years ago.This is a huge pro-growth stimulus for the economy. Every year the Obama WH overstated how the economy would grow. Now real economics and jobs.‚Äù @WSJ Report',\n",
       "       'My deepest condolences to the victims of the terrible shooting in Douglas County @DCSheriff and their families. We love our police and law enforcement - God Bless them all! #LESM',\n",
       "       'Iran the Number One State of Sponsored Terror with numerous violations of Human Rights occurring on an hourly basis has now closed down the Internet so that peaceful demonstrators cannot communicate. Not good!',\n",
       "       'Crooked Hillary Clinton‚Äôs top aid Huma Abedin has been accused of disregarding basic security protocols. She put Classified Passwords into the hands of foreign agents. Remember sailors pictures on submarine? Jail! Deep State Justice Dept must finally act? Also on Comey &amp; others',\n",
       "       'As Americans you need identification sometimes in a very strong and accurate form for almost everything you do.....except when it comes to the most important thing VOTING for the people that run your country. Push hard for Voter Identification!',\n",
       "       'Well now that collusion with Russia is proving to be a total hoax and the only collusion is with Hillary Clinton and the FBI/Russia the Fake News Media (Mainstream) and this phony new book are hitting out at every new front imaginable. They should try winning an election. Sad!',\n",
       "       'The Fake News Awards those going to the most corrupt &amp; biased of the Mainstream Media will be presented to the losers on Wednesday January 17th rather than this coming Monday. The interest in and importance of these awards is far greater than anyone could have anticipated!',\n",
       "       'The fact that Sneaky Dianne Feinstein who has on numerous occasions stated that collusion between Trump/Russia has not been found would release testimony in such an underhanded and possibly illegal way totally without authorization is a disgrace. Must have tough Primary!',\n",
       "       'Democrat Dianne Feinstein should never have released secret committee testimony to the public without authorization. Very disrespectful to committee members and possibly illegal. She blamed her poor decision on the fact she had a cold - a first!',\n",
       "       '‚Äú90% of Trump 2017 news coverage was negative‚Äù -and much of it contrived!@foxandfriends',\n",
       "       'Beautiful weather all over our great country a perfect day for all Women to March. Get out there now to celebrate the historic milestones and unprecedented economic success and wealth creation that has taken place over the last 12 months. Lowest female unemployment in 18 years!',\n",
       "       'Our entire Nation w/one heavy heart continues to pray for the victims &amp; their families in Parkland FL. To teachers law enforcement first responders &amp; medical professionals who responded so bravely in the face of danger: We THANK YOU for your courage! https://t.co/3yJsrebZMG https://t.co/ti791dENTy',\n",
       "       'The Democrat memo response on government surveillance abuses is a total political and legal BUST. Just confirms all of the terrible things that were done. SO ILLEGAL!',\n",
       "       'RT @FoxNews: President @realDonaldTrump on DACA: \"I\\'m the one that\\'s pushing DACA and the Democrats are nowhere to be found.\" https://t.co/‚Ä¶',\n",
       "       \"‚ÄúAmerican consumers are the most confident they've been since 2000....A strong job market is boosting confidence. The unemployment rate has stayed at a 17-year low.‚Äù https://t.co/aL7aVoR7XC\",\n",
       "       'When a country Taxes our products coming in at say 50% and we Tax the same product coming into our country at ZERO not fair or smart. We will soon be starting RECIPROCAL TAXES so that we will charge the same thing as they charge us. $800 Billion Trade Deficit-have no choice!',\n",
       "       'Great couple great book! https://t.co/cLDI79rin8',\n",
       "       'North Korea has not conducted a Missile Test since November 28 2017 and has promised not to do so through our meetings. I believe they will honor that commitment!',\n",
       "       'Got $1.6 Billion to start Wall on Southern Border rest will be forthcoming. Most importantly got $700 Billion to rebuild our Military $716 Billion next year...most ever. Had to waste money on Dem giveaways in order to take care of military pay increase and new equipment.',\n",
       "       'As a candidate I pledged that if elected I would use every lawful tool to combat unfair trade protect American workers and defend our national security.  Today we took another critical step to fulfill that commitment. https://t.co/7NBI0Dibmx https://t.co/nmzqos3BUA',\n",
       "       '....lawyer or law firm will take months to get up to speed (if for no other reason than they can bill more) which is unfair to our great country - and I am very happy with my existing team. Besides there was NO COLLUSION with Russia except by Crooked Hillary and the Dems!',\n",
       "       'The United States hasn‚Äôt had a Trade Surplus with China in 40 years. They must end unfair trade take down barriers and charge only Reciprocal Tariffs. The U.S. is losing $500 Billion a year and has been losing Billions of Dollars for decades. Cannot continue!',\n",
       "       'We are sealing up our Southern Border. The people of our great country want Safety and Security. The Dems have been a disaster on this very important issue!',\n",
       "       'Many dead including women and children in mindless CHEMICAL attack in Syria. Area of atrocity is in lockdown and encircled by Syrian Army making it completely inaccessible to outside world. President Putin Russia and Iran are responsible for backing Animal Assad. Big price...',\n",
       "       'Much of the bad blood with Russia is caused by the Fake &amp; Corrupt Russia Investigation headed up by the all Democrat loyalists or people that worked for Obama. Mueller is most conflicted of all (except Rosenstein who signed FISA &amp; Comey letter). No Collusion so they go crazy!',\n",
       "       'Looks like Jerry Brown and California are not looking for safety and security along their very porous Border. He cannot come to terms for the National Guard to patrol and protect the Border. The high crime rate will only get higher. Much wanted Wall in San Diego already started!',\n",
       "       'THANK YOU #JIATFSouth @Norad_Northcom @southcomwatch and @DHSgov. Keep up the GREAT work! https://t.co/3v2uG6Jp1T',\n",
       "       'James Comey Memos just out and show clearly that there was NO COLLUSION and NO OBSTRUCTION. Also he leaked classified information. WOW! Will the Witch Hunt continue?',\n",
       "       'Funny how all of the Pundits that couldn‚Äôt come close to making a deal on North Korea are now all over the place telling me how to make a deal!',\n",
       "       'Look forward to meeting with Chancellor Angela Merkel of Germany today. So much to discuss so little time! It will be good for both of our great countries!',\n",
       "       'So sad to see the Terror Attack in Paris. At some point countries will have to open their eyes &amp; see what is really going on. This kind of sickness &amp; hatred is not compatible with a loving peaceful &amp; successful country! Changes to our thought process on terror must be made.',\n",
       "       'Gina Haspel is one step closer to leading our brave men and women at the CIA. She is exceptionally qualified and the Senate should confirm her immediately. We need her to keep our great country safe! #ConfirmGina',\n",
       "       '‚ÄúApparently the DOJ put a Spy in the Trump Campaign. This has never been done before and by any means necessary they are out to frame Donald Trump for crimes he didn‚Äôt commit.‚Äù  David Asman  @LouDobbs @GreggJarrett   Really bad stuff!',\n",
       "       '....At what point does this soon to be $20000000 Witch Hunt composed of 13 Angry and Heavily Conflicted Democrats and two people who have worked for Obama for 8 years STOP! They have found no Collussion with Russia No Obstruction but they aren‚Äôt looking at the corruption...',\n",
       "       'China must continue to be strong &amp; tight on the Border of North Korea until a deal is made. The word is that recently the Border has become much more porous and more has been filtering in. I want this to happen and North Korea to be VERY successful but only after signing!',\n",
       "       '...but complain and obstruct. They made only bad deals (Iran) and their so-called Trade Deals are the laughing stock of the world!',\n",
       "       'Happy Memorial Day! Those who died for our great country would be very happy and proud at how well our country is doing today. Best economy in decades lowest unemployment numbers for Blacks and Hispanics EVER (&amp; women in 18years) rebuilding our Military and so much more. Nice!',\n",
       "       'With the #RightToTry Law I signed today patients with life threatening illnesses will finally have access to experimental treatments that could improve or even cure their conditions. These are experimental treatments and products that have shown great promise... https://t.co/FIUwhpUpoL',\n",
       "       'Chris Farrell Judicial Watch. ‚ÄúThey were running an operation to undermine a candidate for President of the U.S. These are all violations of law. This is intelligence tradecraft to steer an election. There‚Äôs nothing more grave when it comes to abuse of our intelligence system...',\n",
       "       'Stock Market up almost 40% since the Election with 7 Trillion Dollars of U.S. value built throughout the economy. Lowest unemployment rate in many decades with Black &amp; Hispanic unemployment lowest in History and Female unemployment lowest in 21 years. Highest confidence ever!',\n",
       "       '...Got along great with Kim Jong-un who wants to see wonderful things for his country. As I said earlier today: Anyone can make war but only the most courageous can make peace! #SingaporeSummit',\n",
       "       'A year ago the pundits &amp; talking heads people that couldn‚Äôt do the job before were begging for conciliation and peace - ‚Äúplease meet don‚Äôt go to war.‚Äù Now that we meet and have a great relationship with Kim Jong Un the same haters shout out ‚Äúyou shouldn‚Äôt meet do not meet!‚Äù',\n",
       "       'RT @foxandfriends: .@jasoninthehouse: Anything Mueller is doing with his investigation is tainted by the anti-Trump FBI agents https://t.co‚Ä¶',\n",
       "       'Elect more Republicans in  November and we will pass the finest fairest and most comprehensive Immigration Bills anywhere in the world. Right now we have the dumbest and the worst. Dems are doing nothing but Obstructing. Remember their motto RESIST! Ours is PRODUCE!',\n",
       "       'Based on the Tariffs and Trade Barriers long placed on the U.S. and it great companies and workers by the European Union if these Tariffs and Barriers are not soon broken down and removed we will be placing a 20% Tariff on all of their cars coming into the U.S. Build them here!',\n",
       "       'Based on the Tariffs and Trade Barriers long placed on the U.S. &amp; its great companies and workers by the European Union if these Tariffs and Barriers are not soon broken down and removed we will be placing a 20% Tariff on all of their cars coming into the U.S. Build them here!',\n",
       "       'RT @realDonaldTrump: Elect more Republicans in  November and we will pass the finest fairest and most comprehensive Immigration Bills anyw‚Ä¶',\n",
       "       'Hiring manythousands of judges and going through a long and complicated legal process is not the way to go - will always be disfunctional. People must simply be stopped at the Border and told they cannot come into the U.S. illegally. Children brought back to their country......',\n",
       "       '....If this is done illegal immigration will be stopped in it‚Äôs tracks - and at very little by comparison cost. This is the only real answer - and we must continue to BUILD THE WALL!',\n",
       "       'Tremendous win for Congressman Dan Donovan. You showed great courage in a tough race! New York and my many friends on Staten Island have elected someone they have always been very proud of. Congratulations!',\n",
       "       'Six months after our TAX CUTS more than 6 MILLION workers have received bonuses pay raises and retirement account contributions.#TaxCutsandJobsAct https://t.co/MevjwIINGU',\n",
       "       'I never pushed the Republicans in the House to vote for the Immigration Bill either GOODLATTE 1 or 2 because it could never have gotten enough Democrats as long as there is the 60 vote threshold. I released many prior to the vote knowing we need more Republicans to win in Nov.',\n",
       "       'Crazy Maxine Waters said by some to be one of the most corrupt people in politics is rapidly becoming together with Nancy Pelosi the FACE of the Democrat Party. Her ranting and raving even referring to herself as a wounded animal will make people flee the Democrats!',\n",
       "       'The economy is doing perhaps better than ever before and that‚Äôs prior to fixing some of the worst and most unfair Trade Deals ever made by any country. In any event they are coming along very well. Most countries agree that they must be changed but nobody ever asked!',\n",
       "       'Wow! The NSA has deleted 685 million phone calls and text messages. Privacy violations? They blame technical irregularities. Such a disgrace. The Witch Hunt continues!',\n",
       "       'Many countries in NATO which we are expected to defend are not only short of their current commitment of 2% (which is low) but are also delinquent for many years in payments that have not been made. Will they reimburse the U.S.?',\n",
       "       '‚ÄúTrump has been the most consequential president in history when it comes to minority employment. In June for instance the unemployment rate for Hispanics and Latinos 16 years and older fell to 4.6% its lowest level ever from 4.9% in May.‚Äù https://t.co/ex9jizOyAV',\n",
       "       'RT @realDonaldTrump: ‚ÄúTrump has been the most consequential president in history when it comes to minority employment. In June for instanc‚Ä¶',\n",
       "       'President Obama thought that Crooked Hillary was going to win the election so when he was informed by the FBI about Russian Meddling he said it couldn‚Äôt happen was no big deal &amp; did NOTHING about it. When I won it became a big deal and the Rigged Witch Hunt headed by Strzok!',\n",
       "       '‚ÄúA lot of Democrats wished they voted for the Tax Cuts because the economy is booming - we could have 4% growth now and the Fed said yesterday that unemployment could drop again.‚Äù  @foxandfriends  @kilmeade',\n",
       "       '3.4 million jobs created since our great Election Victory - far greater than ever anticipated and only getting better as new and greatly improved Trade Deals start coming to fruition!',\n",
       "       'The Fake News Media wants so badly to see a major confrontation with Russia even a confrontation that could lead to war. They are pushing so recklessly hard and hate the fact that I‚Äôll probably have a good relationship with Putin. We are doing MUCH better than any other country!',\n",
       "       'I told you so! The European Union just slapped a Five Billion Dollar fine on one of our great companies Google. They truly have taken advantage of the U.S. but not for long!',\n",
       "       '....The United States should not be penalized because we are doing so well. Tightening now hurts all that we have done. The U.S. should be allowed to recapture what was lost due to illegal currency manipulation and BAD Trade Deals. Debt coming due &amp; we are raising rates - Really?',\n",
       "       'RT @realDonaldTrump: I told you so! The European Union just slapped a Five Billion Dollar fine on one of our great companies Google. They‚Ä¶',\n",
       "       'Congratulations to @JudicialWatch and @TomFitton on being successful in getting the Carter Page FISA documents. As usual they are ridiculously heavily redacted but confirm with little doubt that the Department of ‚ÄúJustice‚Äù and FBI misled the courts. Witch Hunt Rigged a Scam!',\n",
       "       '.@PeteHegseth on @FoxNews  ‚ÄúSource #1 was the (Fake) Dossier. Yes the Dirty Dossier paid for by Democrats as a hit piece against Trump and looking for information that could discredit Candidate #1 Trump. Carter Page was just the foot to surveil the Trump campaign...‚Äù ILLEGAL!',\n",
       "       '‚ÄúThe Russia Hoax The Illicit Scheme To Clear Hillary Clinton &amp; Frame Donald Trump‚Äù is a Hot Seller already Number One! More importantly it is a great book that everyone is talking about. It covers the Rigged Witch Hunt brilliantly. Congratulations to Gregg Jarrett!',\n",
       "       'When you have people snipping at your heels during a negotiation it will only take longer to make a deal and the deal will never be as good as it could have been with unity. Negotiations are going really well be cool. The end result will be worth it!',\n",
       "       'Thank you Georgia! They say that my endorsement last week of Brian Kemp in the Republican Primary for Governor against a very worthy opponent lifted him from 5 points down to a 70% to 30% victory! Two very good and talented men in a great race but congratulations to Brian!',\n",
       "       '....the only Collusion with Russia was with the Democrats so now they are looking at my Tweets (along with 53 million other people) - the rigged Witch Hunt continues! How stupid and unfair to our Country....And so the Fake News doesn‚Äôt waste my time with dumb questions NO....',\n",
       "       '...accurately. 90% of media coverage of my Administration is negative despite the tremendously positive results we are achieving it‚Äôs no surprise that confidence in the media is at an all time low! I will not allow our great country to be sold out by anti-Trump haters in the...',\n",
       "       '..This is a terrible situation and Attorney General Jeff Sessions should stop this Rigged Witch Hunt right now before it continues to stain our country any further. Bob Mueller is totally conflicted and his 17 Angry Democrats that are doing his dirty work are a disgrace to USA!',\n",
       "       'Ohio vote today for Troy Balderson for Congress. His opponent controlled by Nancy Pelosi is weak on Crime the Border Military Vets your 2nd Amendment - and will end your Tax Cuts. Troy will be a great Congressman. #MAGA',\n",
       "       'Congratulations to a future STAR of the Republican Party future Senator John James. A big and bold victory tonight in the Great State of Michigan - the first of many. November can‚Äôt come fast enough!',\n",
       "       'The riots in Charlottesville a year ago resulted in senseless death and division. We must come together as a nation. I condemn all types of racism and acts of violence. Peace to ALL Americans!',\n",
       "       '.....released in 2017. If his statement is based on intelligence he has seen since leaving office it constitutes an intelligence breach......‚Äù  Richard Burr (R-NC) Senate Intel Cmte Chair  @LouDobbs',\n",
       "       '....attend the big parade already scheduled at Andrews Air Force Base on a different date &amp; go to the Paris parade celebrating the end of the War on November 11th. Maybe we will do something next year in D.C. when the cost comes WAY DOWN. Now we can buy some more jet fighters!',\n",
       "       'I am sorry to have to reiterate that there are serious and unpleasant consequences to crossing the Border into the United States ILLEGALLY! If there were no serious consequences our country would be overrun with people trying to get in and our system could not handle it!',\n",
       "       'Longest bull run in the history of the stock market congratulations America!',\n",
       "       'RT @DanScavino: ‚Äú2020 odds: @realDonaldTrump favored more than every other challenger combined‚Äù via @SecretsBedard @dcexaminer https://t.co‚Ä¶',\n",
       "       'Target CEO raves about the Economy. ‚ÄúThis is the best consumer environment I‚Äôve seen in my career.‚Äù A big statement from a top executive. But virtually everybody is saying this &amp; when our Trade Deals are made &amp; cost cutting done you haven‚Äôt seen anything yet!  @DRUDGE_REPORT',\n",
       "       'Social Media Giants are silencing millions of people. Can‚Äôt do this even if it means we must continue to hear Fake News like CNN whose ratings have suffered gravely. People have to figure out what is real and what is not without censorship!',\n",
       "       'RT @realDonaldTrump: Social Media Giants are silencing millions of people. Can‚Äôt do this even if it means we must continue to hear Fake New‚Ä¶',\n",
       "       'The Rigged Russia Witch Hunt did not come into play even a little bit with respect to my decision on Don McGahn!',\n",
       "       'Report: There were no FISA hearings held over Spy documents.‚ÄùIt is astonishing that the FISA courts couldn‚Äôt hold hearings on Spy Warrants targeting Donald Trump. It isn‚Äôt about Carter Page it‚Äôs about the Trump Campaign. You‚Äôve got corruption at the DOJ &amp; FBI. The leadership....',\n",
       "       '....of the DOJ &amp; FBI are completely out to lunch in terms of exposing and holding those accountable who are responsible for that corruption.‚Äù @TomFitton @JudicialWatch',\n",
       "       'So true! ‚ÄúMr. Trump remains the single most popular figure in the Republican Party whose fealty has helped buoy candidates in competitive Republican primaries and remains a hot commodity among general election candidates.‚Äù  Nicholas Fandos @nytimes',\n",
       "       '‚ÄúIt is mostly anonymous sources in here why should anyone trust you? General Mattis General Kelly said it‚Äôs not true.‚Äù @SavannahGuthrie  @TODAYshow  Bob Woodward is a liar who is like a Dem operative prior to the Midterms. He was caught cold even by NBC.',\n",
       "       'New Strzok-Page texts reveal ‚ÄúMedia Leak Strategy.‚Äù  @FoxNews  So terrible and NOTHING is being done at DOJ or FBI - but the world is watching and they get it completely.',\n",
       "       '‚ÄúThe story of Puerto Rico is the rebuilding that has occurred. The President has done an extraordinary job of cleanup rebuilding electrical stuff and everything else.‚Äù  @EdRollins   ‚ÄúThe people of Puerto Rico have one of the most corrupt governments in our country.‚Äù @LouDobbs',\n",
       "       'The illegal Mueller Witch Hunt continues in search of a crime. There was never Collusion with Russia except by the Clinton campaign so the 17 Angry Democrats are looking at anything they can find. Very unfair and BAD for the country. ALSO not allowed under the LAW!',\n",
       "       'RT @Norad_Northcom: \"I would like to highlight the incredible work that is being done by the state &amp; local first responders. We are posture‚Ä¶',\n",
       "       'Tariffs have put the U.S. in a very strong bargaining position with Billions of Dollars and Jobs flowing into our Country - and yet cost increases have thus far been almost unnoticeable. If countries will not make fair deals with us they will be ‚ÄúTariffed!‚Äù',\n",
       "       'I want to know where is the money for Border Security and the WALL in this ridiculous Spending Bill and where will it come from after the Midterms? Dems are obstructing Law Enforcement and Border Security. REPUBLICANS MUST FINALLY GET TOUGH!',\n",
       "       'The crowd in front of the U.S. Supreme Court is tiny looks like about 200 people (&amp; most are onlookers) - that wouldn‚Äôt even fill the first couple of rows of our Kansas Rally or any of our Rallies for that matter! The Fake News Media tries to make it look sooo big &amp; it‚Äôs not!',\n",
       "       'We are with you Florida! https://t.co/qzrVLeFbyFhttps://t.co/HVVhSmBg7S https://t.co/rcB6OCwLeH',\n",
       "       'RT @fema: It‚Äôs extremely important to keep sheltering in a safe place and to stay aware as #Michael continues to move inland! https://t.co/‚Ä¶',\n",
       "       '...during the call and told me that he has already started and will rapidly expand a full and complete investigation into this matter. Answers will be forthcoming shortly.',\n",
       "       'Beto O‚ÄôRourke is a total lightweight compared to Ted Cruz and he comes nowhere near representing the values and desires of the people of the Great State of Texas. He will never be allowed to turn Texas into Venezuela!',\n",
       "       'Ron @RonDeSantisFL DeSantis is working hard. A great Congressman and top student at Harvard &amp; Yale Ron will be a record setting governor for Florida. Rick Scott gave him tremendous foundations to further build on. His opponent runs one of the worst &amp; most corrupt cities in USA!',\n",
       "       'Ron @RonDeSantisFL DeSantis had a great debate victory tonight against Andrew Gillum a mayor who presides over one of the worst run and most corrupt cities in Florida. Ron will build on the great job done by Governor Rick Scott. Gillum will make Florida the next Venezuela!',\n",
       "       'We send our deepest condolences to Lou and the entire Barletta family - he has been working so hard despite this terrible situation for the people of Pennsylvania. Our thoughts and prayers are with you Lou!',\n",
       "       'We send our deepest condolences to @RepLouBarletta and the entire Barletta family on the passing of his brother. Lou has been working so hard despite this terrible situation for the people of Pennsylvania. Our thoughts and prayers are with Lou and the entire Barletta family!',\n",
       "       'The safety of the American People is my highest priority. I have just concluded a briefing with the FBI Department of Justice Department of Homeland Security and the U.S. Secret Service... https://t.co/nEUBcq4NOh',\n",
       "       'The United States has been spending Billions of Dollars a year on Illegal Immigration. This will not continue. Democrats must give us the votes to pass strong (but fair) laws. If not we will be forced to play a much tougher hand.',\n",
       "       'It is my great honor to be with so many brilliant courageous patriotic and PROUD AMERICANS. Seeing all of you here today fills me with extraordinary confidence in America‚Äôs future. Each of you is taking part in the Young Black Leadership Summit because you are true leaders... https://t.co/lIlZBxKjuG',\n",
       "       'In Florida there is a choice between a Harvard/Yale educated man named @RonDeSantisFL who has been a great Congressman and will be a great Governor - and a Dem who is a thief and who is Mayor of poorly run Tallahassee said to be one of the most corrupt cities in the Country!',\n",
       "       'Our military is being mobilized at the Southern Border. Many more troops coming. We will NOT let these Caravans which are also made up of some very bad thugs and gang members into the U.S. Our Border is sacred must come in legally. TURN AROUND!',\n",
       "       'So-called Birthright Citizenship which costs our Country billions of dollars and is very unfair to our citizens will be ended one way or the other. It is not covered by the 14th Amendment because of the words ‚Äúsubject to the jurisdiction thereof.‚Äù Many legal scholars agree.....',\n",
       "       'Ron DeSantis showed great courage in his hard fought campaign to become the Governor of Florida. Congratulations to Ron and family!',\n",
       "       'Poland a great country - Congratulations on the 100th Anniversary of your Independence. I will never forget my time there! https://t.co/gEme6McF1x',\n",
       "       '.....hundreds of billions of dollars for the great privilege of losing hundreds of billions of dollars with these same countries on trade. I told them that this situation cannot continue - It is and always has been ridiculously unfair to the United States. Massive amounts.....',\n",
       "       'Brutal and Extended Cold Blast could shatter ALL RECORDS - Whatever happened to Global Warming?',\n",
       "       'The Phony Witch Hunt continues but Mueller and his gang of Angry Dems are only looking at one side not the other. Wait until it comes out how horribly &amp; viciously they are treating people ruining lives for them refusing to lie. Mueller is a conflicted prosecutor gone rogue....',\n",
       "       '....terrible Gang of Angry Democrats. Look at their past and look where they come from. The now $30000000 Witch Hunt continues and they‚Äôve got nothing but ruined lives. Where is the Server? Let these terrible people go back to the Clinton Foundation and ‚ÄúJustice‚Äù Department!',\n",
       "       '.@StephenMoore and Arthur Laffer two very talented men have just completed an incredible book on my Economic Policies or as they call it #TRUMPONOMICS....',\n",
       "       'Trish_Regan:  ‚ÄúDid the FBI follow protocol to obtain the FISA warrant? I don‚Äôt think so. The Dossier was opposition research funded by opponents. Don‚Äôt use Government resources to take down political foes. Weaponizing Government for gain.‚Äù Is this really America? Witch Hunt!',\n",
       "       '....Will Robert Mueller‚Äôs big time conflicts of interest be listed at the top of his Republicans only Report. Will Andrew Weissman‚Äôs horrible and vicious prosecutorial past be listed in the Report. He wrongly destroyed people‚Äôs lives took down great companies only to be........',\n",
       "       'It has been incorrectly reported that Rudy Giuliani and others will not be doing a counter to the Mueller Report. That is Fake News. Already 87 pages done but obviously cannot complete until we see the final Witch Hunt Report.',\n",
       "       'China just announce the there economy is growing much slower than anticipated because of our Trade War with them. They have just suspended U.S. Tariff Hikes. U.S. is doing very well. China wants to make a big and very comprehensive deal. It could happen and rather soon!',\n",
       "       'China just announced that their economy is growing much slower than anticipated because of our Trade War with them. They have just suspended U.S. Tariff Hikes. U.S. is doing very well. China wants to make a big and very comprehensive deal. It could happen and rather soon!',\n",
       "       'Many people have asked how we are doing in our negotiations with North Korea - I always reply by saying we are in no hurry there is wonderful potential for great economic success for that country....',\n",
       "       'In our Country so much money has been poured down the drain for so many years but the Democrats fight us like cats and dogs when it comes to spending on Boarder Security (including a Wall) and the Military. We won on the Military it is being completely rebuilt. We will win...',\n",
       "       'In our Country so much money has been poured down the drain for so many years but when it comes to Border Security and the Military the Democrats fight to the death. We won on the Military which is being completely rebuilt. One way or the other we will win on the Wall!',\n",
       "       'I am in the White House working hard. News reports concerning the Shutdown and Syria are mostly FAKE. We are negotiating with the Democrats on desperately needed Border Security (Gangs Drugs Human Trafficking &amp; more) but it could be a long stay. On Syria we were originally...',\n",
       "       '‚ÄúBorder Patrol Agents want the Wall.‚Äù Democrat‚Äôs say they don‚Äôt want the Wall (even though they know it is really needed) and they don‚Äôt want ICE. They don‚Äôt have much to campaign on do they? An Open Southern Border and the large scale crime that comes with such stupidity!',\n",
       "       'Great work by my Administration over the holidays to save Coast Guard pay during this #SchumerShutdown. No thanks to the Democrats who left town and are not concerned about the safety and security of Americans!',\n",
       "       'Great support coming from all sides for Border Security (including Wall) on our very dangerous Southern Border. Teams negotiating this weekend! Washington Post and NBC reporting of events including Fake sources has been very inaccurate (to put it mildly)!',\n",
       "       'Many people currently a part of my opposition including President Obama &amp; the Dems have had campaign violations in some cases for very large sums of money. These are civil cases. They paid a fine &amp; settled. While no big deal I did not commit a campaign violation!',\n",
       "       'Congratulations to a truly great football team the Clemson Tigers on an incredible win last night against a powerful Alabama team. A big win also for the Great State of South Carolina. Look forward to seeing the team and their brilliant coach for the second time at the W.H.',\n",
       "       'The Fake News Media keeps saying we haven‚Äôt built any NEW WALL. Below is a section just completed on the Border. Anti-climbing feature included. Very high strong and beautiful! Also many miles already renovated and in service! https://t.co/UAAGXl5Byr',\n",
       "       'RT @GOPChairwoman: Smugglers are flooding our communities with drugs.*300 Americans die each week from heroin 90% of it comes from south‚Ä¶',\n",
       "       'Getting ready to address the Farm Convention today in Nashville Tennessee. Love our farmers love Tennessee - a great combination! See you in a little while.',\n",
       "       'I will be making a major announcement concerning the Humanitarian Crisis on our Southern Border and the Shutdown tomorrow afternoon at 3 P.M. live from the @WhiteHouse.',\n",
       "       'Always heard that as President ‚Äúit‚Äôs all about the economy!‚Äù Well we have one of the best economies in the history of our Country. Big GDP lowest unemployment companies coming back to the U.S. in BIG numbers great new trade deals happening &amp; more. But LITTLE media mention!',\n",
       "       'Nancy Pelosi has behaved so irrationally &amp; has gone so far to the left that she has now officially become a Radical Democrat. She is so petrified of the ‚Äúlefties‚Äù in her party that she has lost control...And by the way clean up the streets in San Francisco they are disgusting!',\n",
       "       'RT @senrobportman: .@POTUS has laid out a constructive new proposal that contains the basis for a bipartisan agreement. It includes many of‚Ä¶',\n",
       "       'Last year was the best year for American Manufacturing job growth since 1997 or 21 years. The previous administration said manufacturing will not come back to the U.S. ‚Äúyou would need a magic wand.‚Äù I guess I found the MAGIC WAND - and it is only getting better!',\n",
       "       'A great new book just out ‚ÄúGame of Thorns‚Äù by Doug Wead Presidential Historian and best selling author. The book covers the campaign of 2016 and what could be more exciting than that?',\n",
       "       'A third rate conman who interviewed me many years ago for just a short period of time has been playing his biggest con of all on Fake News CNN. Michael D‚ÄôAntonio a broken down hack who knows nothing about me goes on night after night telling made up Trump stories. Disgraceful!',\n",
       "       'The Intelligence people seem to be extremely passive and naive when it comes to the dangers of Iran. They are wrong! When I became President Iran was making trouble all over the Middle East and beyond. Since ending the terrible Iran Nuclear Deal they are MUCH different but....',\n",
       "       'Just concluded a great meeting with my Intel team in the Oval Office who told me that what they said on Tuesday at the Senate Hearing was mischaracterized by the media - and we are very much in agreement on Iran ISIS North Korea etc. Their testimony was distorted press.... https://t.co/Zl5aqBmpjF',\n",
       "       'Just out: The big deal very mysterious Don jr telephone calls after the innocent Trump Tower meeting that the media &amp; Dems said were made to his father (me) were just conclusively found NOT to be made to me. They were made to friends &amp; business associates of Don. Really sad!',\n",
       "       '....after 18 long years. Syria was loaded with ISIS until I came along. We will soon have destroyed 100% of the Caliphate but will be watching them closely. It is now time to start coming home and after many years spending our money wisely. Certain people must get smart!',\n",
       "       'Great job by law enforcement in Aurora Illinois. Heartfelt condolences to all of the victims and their families. America is with you!',\n",
       "       '....something that is so obviously the future. I want the United States to win through competition not by blocking out currently more advanced technologies. We must always be the leader in everything we do especially when it comes to the very exciting world of technology!',\n",
       "       'RT @seanhannity: **REPORT: ‚ÄúChina is proposing that it could buy an additional $30 billion a year of U.S. agricultural products including s‚Ä¶',\n",
       "       '...Danny‚Äôs recovery reflects the best of what the United States &amp; its partners can accomplish.We work every day to bring Americans home. We maintain constant and intensive diplomatic intelligence and law enforcement cooperation within the United States Government and with...',\n",
       "       'I never like being misinterpreted but especially when it comes to Otto Warmbier and his great family. Remember I got Otto out along with three others. The previous Administration did nothing and he was taken on their watch. Of course I hold North Korea responsible....',\n",
       "       '....President. We are WINNING big the envy of the WORLD but just think what it could be?',\n",
       "       'The military drills or war games as I call them were never even discussed in my mtg w/ Kim Jong Un of NK‚ÄîFAKE NEWS! I made that decision long ago because it costs the U.S. far too much money to have those ‚Äúgames‚Äù especially since we are not reimbursed for the tremendous cost!',\n",
       "       'RT @LouDobbs: #AmericaFirst ‚Äì @marc_lotter: Democrats couldn‚Äôt condemn anti-semitism. That‚Äôs because the radical left-wing socialists have‚Ä¶',\n",
       "       'The Witch Hunt continues! https://t.co/9W1iUgE0d6',\n",
       "       \"RT @TimRunsHisMouth: Democrats so far in 2019:‚úÖWon't condemn anti-semitism in the House of Reps.‚úÖDo nothing against a racist Governor w‚Ä¶\",\n",
       "       'RT @GOPChairwoman: Since the mainstream media doesn‚Äôt cover the booming @realDonaldTrump economy enough:-Wage growth just hit 3.4% ‚Äì the‚Ä¶',\n",
       "       \"RT @paulsperry_: If Schiff wasn't coaching Cohen on how to go after Trump before his testimony and was just going over procedural issues w‚Ä¶\",\n",
       "       '....needed and the complexity creates danger. All of this for great cost yet very little gain. I don‚Äôt know about you but I don‚Äôt want Albert Einstein to be my pilot. I want great flying professionals that are allowed to easily and quickly take control of a plane!',\n",
       "       'New York State and its Governor Andrew Cuomo are now proud members of the group of PRESIDENTIAL HARASSERS. No wonder people are fleeing the State in record numbers. The Witch Hunt continues!',\n",
       "       '‚ÄúJay Leno points out that comedy (on the very boring late night shows) is totally one-sided. It‚Äôs tough when there‚Äôs only one topic.‚Äù @foxandfriends  Actually the one-sided hatred on these shows is incredible and for me unwatchable. But remember WE are number one - President!',\n",
       "       'It‚Äôs truly incredible that shows like Saturday Night Live not funny/no talent can spend all of their time knocking the same person (me) over &amp; over without so much of a mention of ‚Äúthe other side.‚Äù Like an advertisement without consequences. Same with Late Night Shows......',\n",
       "       '‚ÄúThe Special Counsel did not find that the Trump Campaign or anyone associated with it conspired or coordinated with the Russian Government in these efforts despite multiple offers from Russian-affiliated individuals to assist the Trump Campaign.‚Äù',\n",
       "       '‚ÄúI think this is probably the most consequential media screwup of the last 25 to 50 years. It is difficult to comprehend or overstate the damage that the media did to the Country to their own reputation or to the Constitution. An absolute catastrophe‚Äù Sean Davis  @TuckerCarlson',\n",
       "       'The Fake News Media has lost tremendous credibility with its corrupt coverage of the illegal Democrat Witch Hunt of your all time favorite duly elected President me! T.V. ratings of CNN &amp; MSNBC tanked last night after seeing the Mueller Report statement. @FoxNews up BIG!',\n",
       "       'Now that the long awaited Mueller Report conclusions have been released most Democrats and othershave gone back to the pre-Witch Hunt phase of their lives before Collusion Delusion took over. Others are pretending that their former hero Bob Mueller no longer exists!',\n",
       "       'Can you believe that the Radical Left Democrats want to do our new and very important Census Report without the all important Citizenship Question. Report would be meaningless and a waste of the $Billions (ridiculous) that it costs to put together!',\n",
       "       '....The best thing that ever happened to Puerto Rico is President Donald J. Trump. So many wonderful people but with such bad Island leadership and with so much money wasted. Cannot continue to hurt our Farmers and States with these massive payments and so little appreciation!',\n",
       "       '‚ÄúAnd the Radical Dems reeling in disarray as the President continues to Win Win and Win again! The Left is PRETENDING to be shocked by A.G. William Barr‚Äôs testimony before Congress in which he said SPYING did occur on the Trump 2016 Campaign.‚Äù @LouDobbs  @FoxNews',\n",
       "       'A Fake Story by Politico. Made up sources. Thank you Mount Vernon! https://t.co/Pf60zBy6Sw',\n",
       "       'Just signed a critical bill to formalize drought contingency plans for the Colorado River. Thanks to @SenMcSallyAZ for getting it done. Big deal for Arizona!',\n",
       "       'RT @RepMattGaetz: ‚ÄúYou have guys that come through here that do this for a living‚Ä¶they bring groups across they bring drugs across.‚ÄùSher‚Ä¶',\n",
       "       'Good night of television! 8:00 PM @WattersWorld  9:00 PM @JudgeJeanine with interview of @RudyGiuliani followed by Greg G. Honest commentary is always nice!',\n",
       "       'Heartfelt condolences from the people of the United States to the people of Sri Lanka on the horrible terrorist attacks on churches and hotels that have killed at least 138 million people and badly injured 600 more. We stand ready to help!',\n",
       "       '138 people have been killed in Sri Lanka with more that 600 badly injured in a terrorist attack on churches and hotels. The United States offers heartfelt condolences to the great people of Sri Lanka. We stand ready to help!',\n",
       "       'Can you believe that I had to go through the worst and most corrupt political Witch Hunt in the history of the United States (No Collusion) when it was the ‚Äúother side‚Äù that illegally created the diversionary &amp; criminal event and even spied on my campaign? Disgraceful!',\n",
       "       'Only high crimes and misdemeanors can lead to impeachment. There were no crimes by me (No Collusion No Obstruction) so you can‚Äôt impeach. It was the Democrats that committed the crimes not your Republican President! Tables are finally turning on the Witch Hunt!',\n",
       "       'RT @MariaBartiromo: Rep. Ratcliffe: Mueller report proves Donald Trump was telling the truth about collusion https://t.co/dD5kCNBwKl  @Sund‚Ä¶',\n",
       "       'RT @DevinNunes: Victor Davis Hanson: Mueller Probe Could Backfire on Those Who Fabricated Russia-Collusion Narrative - YouTube https://t.co‚Ä¶',\n",
       "       'RT @WhiteHouse: President @realDonaldTrump and @FLOTUS traveled to Atlanta Georgia yesterday for the Rx Drug Abuse and Heroin Summit con‚Ä¶',\n",
       "       'RT @WhiteHouse: Today is National Drug Take Back Day! Find a collection site near you and join the fight against opioid abuse. https://t.co‚Ä¶',\n",
       "       'RT @FLOTUS: #TakeBackDay is a reminder to dispose of your unused prescription drugs responsibly! Find your nearest collection center at htt‚Ä¶',\n",
       "       'I spoke at length yesterday to Rabbi Yisroel Goldstein Chabad  of Poway where I extended my warmest condolences to him and all affected by the shooting in California. What a great guy. He had a least one finger blown off and all he wanted to do is help others. Very special!',\n",
       "       '‚ÄúThe Democrats can‚Äôt come to grips with the fact that there was No Collusion there was No Conspiracy there was No Obstruction. What we should be focused on is what‚Äôs been going on in our government at the highest levels of the FBI....‚Äù Senator Josh Hawley',\n",
       "       'NO COLLUSION NO OBSTRUCTION. Besides how can you have Obstruction when not only was there No Collusion (by Trump) but the bad actions were done by the ‚Äúother‚Äù side? The greatest con-job in the history of American Politics!',\n",
       "       'As we unite on this day of prayer we renew our resolve to protect communities of faith ‚Äì and ensure that ALL of our people can live pray and worship IN PEACE. #NationalDayOfPrayer https://t.co/1a7zphaE6z',\n",
       "       '...at every turn in attempts to gain access. But now Republicans and Democrats must come together for the good of the American people. No more costly &amp; time consuming investigations. Lets do Immigration (Border) Infrastructure much lower drug prices &amp; much more - and do it now!',\n",
       "       'RT @Lauren_Southern: Lmao at establishment conservatives who think they won‚Äôt be labeled the new ‚Äúdangerous‚Äù / ‚Äúextremist‚Äù voices when thos‚Ä¶',\n",
       "       'RT @charliekirk11: BOOM:Despite horrendous media treatment coverage Trump stands at 51% approval rating!They tried to defame himThey‚Ä¶',\n",
       "       '....of additional goods sent to us by China remain untaxed but will be shortly at a rate of 25%. The Tariffs paid to the USA have had little impact on product cost mostly borne by China. The Trade Deal with China continues but too slowly as they attempt to renegotiate. No!',\n",
       "       '....to testify. Are they looking for a redo because they hated seeing the strong NO COLLUSION conclusion? There was no crime except on the other side (incredibly not covered in the Report) and NO OBSTRUCTION. Bob Mueller should not testify. No redos for the Dems!',\n",
       "       'RT @TomFitton: BREAKING: New emails show Obama WH orchestrated Clinton email coverup; Illegal spying on @realDonaldTrump biggest corruption‚Ä¶',\n",
       "       'RT @RudyGiuliani: The evidence is mounting that Comey deceived the FISA court concerning the Steele dossier. State did a quick verification‚Ä¶',\n",
       "       'RT @LindseyGrahamSC: @realDonaldTrump When it comes to China we must be willing to accept short term pain for long term gain. Stick to y‚Ä¶',\n",
       "       'RT @LindseyGrahamSC: When it comes to China they will never change their behavior until someone stands up to them.  I‚Äôm proud of President‚Ä¶',\n",
       "       \"RT @DevinNunes: FBI's Steele story falls apart: False intel and media contacts were flagged before FISA | \\u2066@jsolomonReports\\u2069  https://t.co/‚Ä¶\",\n",
       "       'I say openly to President Xi &amp; all of my many friends in China that China will be hurt very badly if you don‚Äôt make a deal because companies will be forced to leave China for other countries. Too expensive to buy in China. You had a great deal almost completed &amp; you backed out!',\n",
       "       'Our great Senator (and Star) from the State of Arkansas @TomCottonAR has just completed a wonderful book ‚ÄúSacred Duty‚Äù about Arlington National Cemetary and the men and women who serve with such love and devotion. On sale today make it big!',\n",
       "       'Our great Senator (and Star) from the State of Arkansas @TomCottonAR has just completed a wonderful book ‚ÄúSacred Duty‚Äù about Arlington National Cemetery and the men and women who serve with such love and devotion. On sale today make it big!',\n",
       "       'After two years of an expensive and comprehensive Witch Hunt the Democrats don‚Äôt like the result and they want a DO OVER. In other words the Witch Hunt continues!',\n",
       "       'Back from Japan after a very successful trip. Big progress on MANY fronts. A great country with a wonderful leader in Prime Minister Abe!',\n",
       "       'Congressman John Ratcliffe ‚ÄúThe Trump Campaign clearly did not conspire or collude.‚Äù @FoxNews',\n",
       "       'Spoke to Virginia Governor @RalphNortham last night and the Mayor and Vice Mayor of Virginia Beach this morning to offer condolences to that great community. The Federal Government is there and will be for whatever they may need. God bless the families and all!',\n",
       "       'Washington Post got it wrong as usual. The U.S. is charging 25% against 250 Billion Dollars of goods shipped from China not 200 BD. Also China is paying a heavy cost in that they will subsidize goods to keep them coming devalue their currency yet companies are moving to.....',\n",
       "       'I never called Meghan Markle ‚Äúnasty.‚Äù Made up by the Fake News Media and they got caught cold! Will @CNN @nytimes and others apologize? Doubt it!',\n",
       "       '.@FLOTUS Melania and I send our deepest condolences to President Reuven Rivlin and the entire State of Israel upon the passing of Mrs. Nechama Rivlin. Mrs. Rivlin represented her beloved country with grace and stature. We will miss her along with all those who knew her.',\n",
       "       '.....not mentioned in yesterday press release one in particular were agreed upon. That will be announced at the appropriate time. There is now going to be great cooperation between Mexico &amp; the USA something that didn‚Äôt exist for decades. However if for some unknown reason...',\n",
       "       '....We do not anticipate a problem with the vote but if for any reason the approval is not forthcoming Tariffs will be reinstated!',\n",
       "       '...Companies to come to the U.S.A and to get companies that have left us for other lands to come back home. We stupidly lost 30% of our auto business to Mexico. If the Tariffs went on at the higher level they would all come back and pass. But very happy with the deal I made...',\n",
       "       '‚ÄúSomeone should call Obama up. The Obama Administration spied on a rival presidential campaign using Federal Agencies. I mean that seems like a headline to me?‚Äù @TuckerCarlson  It will all start coming out and the Witch Hunt will end. Presidential Harassment!',\n",
       "       \"RT @mike_pence: The American people will continue to be drawn to @realDonaldTrump's consistent conservative leadership - and I couldn't be‚Ä¶\",\n",
       "       'Just spoke to Marillyn Hewson CEO of @LockheedMartin about continuing operations for the @Sikorsky in Coatesville Pennsylvania. She will be taking it under advisement and will be  making a decision soon....',\n",
       "       'RT @CLewandowski_: Great to see a strong showing in NH for @realDonaldTrump on his Birthday.  Happy Birthday @realDonaldTrump https://t.co/‚Ä¶',\n",
       "       'On no issue are Democrats more extreme ‚Äì and more depraved ‚Äì than when it comes to Border Security. The Democrat Agenda of open borders is morally reprehensible. It is the great betrayal of the American Middle Class and our Country as a whole! #Trump2020 https://t.co/f9RJhpp50J',\n",
       "       '....Think of what it could have been if the Fed had gotten it right. Thousands of points higher on the Dow and GDP in the 4‚Äôs or even 5‚Äôs. Now they stick like a stubborn child when we need rates cuts &amp; easing to make up for what other countries are doing against us. Blew it!',\n",
       "       '....Think of what it could have been if the Fed had gotten it right. Thousands of points higher on the Dow and GDP in the 4‚Äôs or even 5‚Äôs. Now they stick like a stubborn child when we need rates cuts &amp; easing to make up for what other countries are doing against us. Blew it!',\n",
       "       'RT @PressSec: Incredible to see all in attendance to hear our @POTUS speak &amp; celebrate our great nation‚Äôs Independence Day! üá∫üá∏ https://t.co‚Ä¶',\n",
       "       '....As well as we are doing from the day after the great Election when the Market shot right up it could have been even better - massive additional wealth would have been created &amp; used very well. Our most difficult problem is not our competitors it is the Federal Reserve!',\n",
       "       'The Dems Witch Hunt continues! https://t.co/jHEPN84IGK',\n",
       "       '....Thank you to Lockheed Martin one of the USA‚Äôs truly great companies!',\n",
       "       \"RT @JamesOKeefeIII: .@realDonaldTrump: He's not controversial he's truthful. He's truthful. https://t.co/5dRXcLpImh\",\n",
       "       '....companies to come to the USA and to get companies that have left us for other lands to COME BACK HOME. We  stupidly lost 30% of our auto business to Mexico. If the Tariffs went on at the higher level they would all come back and fast. But very happy with the deal I made...',\n",
       "       'So interesting to see ‚ÄúProgressive‚Äù Democrat Congresswomen who originally came from countries whose governments are a complete and total catastrophe the worst most corrupt and inept anywhere in the world (if they even have a functioning government at all) now loudly......',\n",
       "       'RT @mhmarsh82: @usminority Thanks Brandon! Keep it coming!',\n",
       "       'Those Tweets were NOT Racist. I don‚Äôt have a Racist bone in my body! The so-called vote to be taken is a Democrat con game. Republicans should not show ‚Äúweakness‚Äù and fall into their trap. This should be a vote on the filthy language statements and lies told by the Democrat.....',\n",
       "       'Because of the faulty thought process we have going for us at the Federal Reserve we pay much higher interest rates than countries that are no match for us economically. In other words our interest costs are much higher than other countries when they should be lower. Correct!',\n",
       "       \"RT @GOPChairwoman: The mainstream media won't cover it nearly enough but thanks to the #FirstStepAct that @realDonaldTrump signed into law‚Ä¶\",\n",
       "       '....as we have done it could have been soooo much better. Interest rate costs should have been much lower &amp; GDP &amp; our Country‚Äôs wealth accumulation much higher. Such a waste of time &amp; money. Also very unfair that other countries manipulate their currencies and pump money in!',\n",
       "       'The WTO is BROKEN when the world‚Äôs RICHEST countries claim to be developing countries to avoid WTO rules and get special treatment. NO more!!! Today I directed the U.S. Trade Representative to take action so that countries stop CHEATING the system at the expense of the USA!',\n",
       "       'Rep Elijah Cummings has been a brutal bully shouting and screaming at the great men &amp; women of Border Patrol about conditions at the Southern Border when actually his Baltimore district is FAR WORSE and more dangerous. His district is considered the Worst in the USA......',\n",
       "       'China is doing very badly worst year in 27 - was supposed to start buying our agricultural product now - no signs that they are doing so. That is the problem with China they just don‚Äôt come through. Our Economy has become MUCH larger than the Chinese Economy is last 3 years....',\n",
       "       'A great couple! https://t.co/h0o1laZ7e3',\n",
       "       'We cannot let those killed in El Paso Texas and Dayton Ohio die in vain. Likewise for those so seriously wounded. We can never forget them and those many who came before them. Republicans and Democrats must come together and get strong background checks perhaps marrying....',\n",
       "       '....John Deere our car companies &amp; others to compete on a level playing field. With substantial Fed Cuts (there is no inflation) and no quantitative tightening the dollar will make it possible for our companies to win against any competition. We have the greatest companies...',\n",
       "       'In a letter to me sent by Kim Jong Un he stated very nicely that he would like to meet and start negotiations as soon as the joint U.S./South Korea joint exercise are over. It was a long letter much of it complaining about the ridiculous and expensive exercises. It was.....',\n",
       "       'Think how wonderful it is to be able to fight back and show to so many how totally dishonest the Fake News Media really is. It may be the most corrupt and disgusting business (almost) there is! MAKE AMERICA GREAT AGAIN!',\n",
       "       'Just completed a very good meeting on Afghanistan. Many on the opposite side of this 19 year war and us are looking to make a deal - if possible!',\n",
       "       \"RT @WhiteHouse: Thanks to President @realDonaldTrump we're the first country with comprehensive legislation on women's leadership in polit‚Ä¶\",\n",
       "       'Great cohesion inside the Republican Party the best I have ever seen. Despite all of the Fake News my Poll Numbers are great. New internal polls show them to be the strongest we‚Äôve had so far! Think what they‚Äôd be if I got fair media coverage!',\n",
       "       'RT @VP: Traveling to the great state of Michigan with @SecretaryCarson to address the @deteconomicclub about how President @realDonaldTrump‚Ä¶',\n",
       "       'Doing great with China and other Trade Deals. The only problem we have is Jay Powell and the Fed. He‚Äôs like a golfer who can‚Äôt putt has no touch. Big U.S. growth if he does the right thing BIG CUT - but don‚Äôt count on him! So far he has called it wrong and only let us down....',\n",
       "       'Just concluded a very good meeting on preventing Mass Shootings. Talks are ongoing w/ both Republicans &amp; Democrats. We are likewise engaging with lawful gun owners survivors grieving family members law enforcement the NRA mental health professionals and school officials...',\n",
       "       'RT @TomFitton: Biggest story anti-@realDonaldTrump media is hiding: Obama FBI/DOJ and State Dept colluded with Bruce Ohr his wife and Cl‚Ä¶',\n",
       "       'RT @Varneyco: Are #Democrats too extreme when it comes to #GunControl? @DavidAsmanfox is joined by @KimStrassel of @WSJ to discuss how gun‚Ä¶',\n",
       "       \"RT @JudicialWatch: CREDIBLE Evidence Ilhan Omar Married Her Brother--'We're Not Sure Her Last Name is Omar!' WATCH MORE HERE: https://t.co/‚Ä¶\",\n",
       "       'Great respect for the fact that President Xi &amp; his Representatives want ‚Äúcalm resolution.‚Äù So impressed that they are willing to come out &amp; state the facts so accurately. This is why he is a great leader &amp; representing a great country. Talks are continuing!https://t.co/0sotrd1Mzh',\n",
       "       'Puerto Rico is one of the most corrupt places on earth. Their political system is broken and their politicians are either Incompetent or Corrupt. Congress approved Billions of Dollars last time more than anyplace else has ever gotten and it is sent to Crooked Pols. No good!....',\n",
       "       \"RT @NWS: There's an increasing chance for #Dorian to bring a triple-threat of dangers to the Florida east coast...üåälife-threatening storm‚Ä¶\",\n",
       "       'RT @MeetThePress: . @chucktodd asks: How could you say Trump is a free-market conservative as he issues tariffs?@DavidMMcintosh: \"What we‚Ä¶',\n",
       "       'The fact that James Comey was not prosecuted for the absolutely horrible things he did just shows how fair and reasonable Attorney General Bill Barr is. So many people and experts that I have watched and read would have taken an entirely different course. Comey got Lucky!',\n",
       "       'RT @MarshaBlackburn: The independence of our judicial branch is under attack.\\xa0Our Democrat colleagues‚Äô threats to pack the Court are an e‚Ä¶',\n",
       "       'RT @JimInhofe: A historic meeting with U.S. military &amp; Mexican officials protecting both sides of our southern border. Due to joint coopera‚Ä¶',\n",
       "       \"RT @RedCross: When it comes to hurricanes preparation is key. Here's what you can do now to make sure your pets are well taken care of in‚Ä¶\",\n",
       "       'RT @KTLA: Rescue operation underway near Santa Cruz Island involving at least 30 people on a 75-foot boat per @USCGLosAngeleshttps://t.co‚Ä¶',\n",
       "       '....when in fact under certain original scenarios it was in fact correct that Alabama could have received some ‚Äúhurt.‚Äù Always good to be prepared! But the Fake News is only interested in demeaning and belittling. Didn‚Äôt play my whole sentence or statement. Bad people!',\n",
       "       '....when in fact under certain original scenarios it was in fact correct that Alabama could have received some ‚Äúhurt.‚Äù Always good to be prepared! But the Fake News is only interested in demeaning and belittling. Didn‚Äôt play my whole sentence or statement. Bad people!',\n",
       "       '....Instead it turned North and went up the coast where it continues now. In the one model through Florida the Great State of Alabama would have been hit or grazed. In the path it took no. Read my FULL FEMA statement. What I said was accurate! All Fake News in order to demean!',\n",
       "       'The Immigration Law Institute‚Äôs Christopher Hajec says ‚ÄúThe Supreme Court has to look st whether DACA is lawful. What they are looking at now is whether Trump‚Äôs recision of DACA is lawful. Must consider lawfulness of DACA itself. Looks very odd that President Trump doesn‚Äôt.....',\n",
       "       'RT @arielmou: \\u2066#Video of @IvankaTrump\\u2069 dancing with Paraguayan women as her South American trip comes to an end today. #WGDP https://t.co/‚Ä¶',\n",
       "       'When all of the people pushing so hard for Criminal Justice Reform were  unable to come even close to getting it done they came to me as a group and asked for my help. I got it done with a group of Senators &amp; others who would never have gone for it. Obama couldn‚Äôt come close....',\n",
       "       'RT @FEMA_Pete: I met with @NCemergency officials today to discuss @fema support as they assess damage and address needs in hard-hit communi‚Ä¶',\n",
       "       'While I like the Vaping alternative to Cigarettes we need to make sure this alternative is SAFE for ALL! Let‚Äôs get counterfeits off the market and keep young children from Vaping!',\n",
       "       'RT @Jim_Jordan: .@CLewandowski_ worked with the Trump campaign for nearly 2 years. He testified they did not collude or coordinate with Rus‚Ä¶',\n",
       "       'RT @RepMarkMeadows: Twitter friends: give a follow to the newest conservative member of Congress from North Carolina: @RepGregMurphy!',\n",
       "       'All Polls and some brand new Polls show very little support for impeachment. Such a waste of time especially with sooo much good that could be done including prescription drug price reduction healthcare infrastructure etc.',\n",
       "       'Congratulations to my friend @SenShelby our powerful Appropriations Chairman for his hard work on many strong bills that continue to rebuild our military invest in border security and many other priorities. Good work and keep fighting you are winning!',\n",
       "       '.....nice call with with the new President of Ukraine it could not have been better or more honorable and the Fake News Media and Democrats working as a team have fraudulently made it look bad. It wasn‚Äôt bad it was very legal and very good. A continuing Witch Hunt!',\n",
       "       'If that perfect phone call with the President of Ukraine Isn‚Äôt considered appropriate then no future President can EVER again speak to another foreign leader!',\n",
       "       'RT @RNCResearch: Sen. Cruz: Congressional Dems ‚Äúare angry about the 2016 election‚Ä¶angry at the voters‚Äùhttps://t.co/fwMnnRl9pO https://t.co‚Ä¶',\n",
       "       'RT @RNCResearch: Rep. Ratcliffe: Whistleblower‚Äôs account is all ‚Äúsecond‚Äù and ‚Äúthird hand‚Äù informationhttps://t.co/vwEg3FEmO5 https://t.co/‚Ä¶',\n",
       "       'RT @rmooredenton: Mark Levin @marklevinshow really let Ed Henry @EdHenryNews on @foxandfriends have it about corrupt news media.  Ed Henry‚Ä¶',\n",
       "       'RT @DuaneInskeep: I‚Äôve generally been a fan of Ed Henry however Mark Levin just completely destroyed him on Fox and Friends this morning.',\n",
       "       'Like every American I deserve to meet my accuser especially when this accuser the so-called ‚ÄúWhistleblower‚Äù represented a perfect conversation with a foreign leader in a totally inaccurate and fraudulent way. Then Schiff made up what I actually said by lying to Congress......',\n",
       "       '....the Whistleblower and also the person who gave all of the false information to him. This is simply about a phone conversation that could not have been nicer warmer or better. No pressure at all (as confirmed by  Ukrainian Pres.). It is just another Democrat Hoax!',\n",
       "       'RT @DavidAFrench: Biden‚Äôs gun plan? It could destroy the gun industry for selling lawful constitutionally-protected products and impair Am‚Ä¶',\n",
       "       'The Whistleblower who had the facts wrong about the phone call reached out and more to the Democrat controlled House Intelligence Committee. Schiff never told us about this!',\n",
       "       'The first so-called second hand information ‚ÄúWhistleblower‚Äù got my phone conversation almost completely wrong so now word is they are going to the bench and another ‚ÄúWhistleblower‚Äù is coming in from the Deep State also with second hand info. Meet with Shifty. Keep them coming!',\n",
       "       'RT @hughhewitt: I have promoted @KimStrassel book frequently on the air and in two @washingtonpost columns because it persuasively and succ‚Ä¶',\n",
       "       'RT @JohnCornyn: The do it in secret cherry pick and leak whatever furthers their narrative.  Put it out there for all to see. https://t.co‚Ä¶',\n",
       "       'In case the Kurds or Turkey lose control the United States has already taken the 2 ISIS militants tied to beheadings in Syria known as the Beetles out of that country and into a secure location controlled by the U.S. They are the worst of the worst!',\n",
       "       'Dealing with @LindseyGrahamSC and many members of Congress including Democrats about imposing powerful Sanctions on Turkey. Treasury is ready to go additional legislation may be sought. There is great consensus on this. Turkey has asked that it not be done. Stay tuned!',\n",
       "       'Somebody please explain to Chris Wallace of Fox who will never be his father (and my friend) Mike Wallace that the Phone Conversation I had with the President of Ukraine was a congenial &amp; good one. It was only Schiff‚Äôs made up version of that conversation that was bad!',\n",
       "       '....Democrat‚Äôs game was foiled when we caught Schiff fraudulently making up my Ukraine conversation when I released the exact conversation Transcript and when Ukrainian President and the Foreign Minister said there was NO PRESSURE very normal talk! A total Impeachment Scam!',\n",
       "       'You would think there is NO WAY that any of the Democrat Candidates that we witnessed last night could possibly become President of the United States. Now you see why they have no choice but to push a totally illegal &amp; absurd Impeachment of one of the most successful Presidents!',\n",
       "       'Senator Rand Paul just wrote a great book ‚ÄúThe Case Against Socialism‚Äù which is now out. Highly recommended ‚Äì as America was founded on LIBERTY &amp; INDEPENDENCE ‚Äì not government coercion domination &amp; control. We were born free and will stay free as long as I am your President!',\n",
       "       'My warmest condolences to the family and many friends of Congressman Elijah Cummings. I got to see first hand the strength passion and wisdom of this highly respected political leader. His work and voice on so many fronts will be very hard if not impossible to replace!',\n",
       "       'As the Witch Hunt continues! https://t.co/klvvwSXq7M',\n",
       "       'Tonight we forcefully condemn the blatant corruption of the Democrat Party the Fake News Media and the rogue bureaucrats of the Deep State. The only message these radicals will understand is a crushing defeat on November 3 2020! #KAG2020 https://t.co/QW1Rk99O4b',\n",
       "       'Democrats are now the party of high taxes high crime open borders late-term abortion socialism and blatant corruption. The Republican Party is the party of the American Worker the American Family and the American Dream! #KAG2020 https://t.co/qXB198T5cM',\n",
       "       '‚ÄúI don‚Äôt see anything that constitutes an Impeachable offense - Nothing here rises to the level of Impeachment. The Democrats are making a mistake with this secrecy.‚Äù Kenn Starr former Special Prosecutor',\n",
       "       'RT @DonaldJTrumpJr: Ahhhh good times. Happy third anniversary to one of the great comebacks of all time. https://t.co/KbSos1UApS',\n",
       "       '....because their so-called story didn‚Äôt come even close to matching up with the exact transcript of the phone call. Was it a Corrupt Adam Schiff con? Why didn‚Äôt the IG see this? When do we depose Shifty Schiff to find out why he fraudulently made up my phone call and read this..',\n",
       "       'The Witch Hunt continues! https://t.co/pBzhZfJEYb',\n",
       "       '....feel EMPOWERED. There‚Äôs a movement happening on these campuses like I‚Äôve never seen before. When you have 3000 students wanting to get into an event that couldn‚Äôt get in that‚Äôs pretty remarkable!‚Äù  @charliekirk11 Turning Point USA   KEEP AMERICA GREAT!',\n",
       "       'RT @jmclghln: 52% majority says impeachment is political to stop @realDonaldTrump reelection only 36% legal 47%-33% say @POTUS shudn‚Äôt coop‚Ä¶',\n",
       "       'Had a beautiful dinner last night at Camp David in celebration of the 10th Wedding Anniversary of Ivanka and Jared. Attended by a small number of family and friends it could not have been nicer. Camp David is a special place. Cost of the event will be totally paid for by me!',\n",
       "       'The Fake Washington Post keeps doing phony stories with zero sources that I am concerned with the Impeachment scam. I am not because I did nothing wrong. It is the other side including Schiff and his made up story that are concerned. Witch Hunt continues!',\n",
       "       '‚ÄúAmericans know by now that the Impeachment inquiry is just another hoax and silent coup to remove the President from office.‚Äù J.J. Crovatto  Don‚Äôt worry J.J. Schiff is a leaker &amp; corrupt politician who made up what I said on the call in order to hurt the Republican Party &amp; me!',\n",
       "       'Just confirmed that Abu Bakr al-Baghdadi‚Äôs number one replacement has been terminated by American troops. Most likely would have taken the top spot - Now he is also Dead!',\n",
       "       'RT @realDonaldTrump: Just confirmed that Abu Bakr al-Baghdadi‚Äôs number one replacement has been terminated by American troops. Most likely‚Ä¶',\n",
       "       'The Whistleblower must come forward to explain why his account of the phone call with the Ukrainian President was so inaccurate (fraudulent?). Why did the Whistleblower deal with corrupt politician Shifty Adam Schiff and/or his committee?',\n",
       "       'The Whistleblower got it sooo wrong that HE must come forward. The Fake News Media knows who he is but being an arm of the Democrat Party don‚Äôt want to reveal him because there would be hell to pay.  Reveal the Whistleblower and end the Impeachment Hoax!',\n",
       "       '....and FEDERAL CONTRACTOR SPIES stories. The finest law enforcement on the planet could not have shown a ROADMAP like that which was produced by you. @OANN should be VERY proud of this great work. I wish more people were seeking the facts and the truth. Keep it up!',\n",
       "       'Thank you to Kurt Volker U.S. Envoy to Ukraine who said in his Congressional Testimony just released ‚ÄúYou asked what conversations did I have about that quid pro quo et cetra. NONE because I didn‚Äôt know there was a quid pro quo.‚Äù  Witch Hunt!',\n",
       "       'Bill Barr did not decline my request to talk about Ukraine. The story was a Fake Washington Post con job with an ‚Äúanonymous‚Äù source that doesn‚Äôt exist. Just read the Transcript. The Justice Department already ruled that the call was good. We don‚Äôt have freedom of the press!',\n",
       "       'But the Witch Hunt continues. After 3 years of relentless attacks against the Republican Party &amp; me the Do Nothing Dems are losers for America! https://t.co/met2y8VoPe',\n",
       "       'RT @realDonaldTrump: But the Witch Hunt continues. After 3 years of relentless attacks against the Republican Party &amp; me the Do Nothing De‚Ä¶',\n",
       "       'RT @DeptofDefense: Happy 244th birthday üéÇ to the most powerful fighting force on the planet the @USMC!#HappyBirthdayMarines https://t.co/‚Ä¶',\n",
       "       '‚ÄúThe circus is coming to town. The corrupt compromised coward &amp; congenital liar Adam Schiff Show on Capital Hill brought to you by his raging psychotic Democrats &amp; the top allies in the Media Mob. Everything you‚Äôre going to see in the next two weeks is rigged.....',\n",
       "       'RT @RepMarkMeadows: Reminder: the Chairman was caught misleading a reporter about communication with the whistleblower barely four weeks a‚Ä¶',\n",
       "       '‚ÄúThat hearing was about a policy dispute.‚Äù Robert Ray. ‚ÄúRegular people watching this cannot conclude that the President of the United States can be impeached.‚Äù @Varneyco',\n",
       "       '....We send our deepest condolences to the families and friends of those tragically lost and we pray for the speedy recovery of the wounded.',\n",
       "       'We have vacancies in various departments because we do not want or need as many people as past administrations (and save great cost) and also the Democrats delay the approval process to levels unprecedented in the history of our Country!',\n",
       "       'RT @EliseStefanik: Obama‚Äôs own State Dept. was so concerned about conflicts of interest from Hunter Biden‚Äôs role at Burisma that they raise‚Ä¶',\n",
       "       'RT @HouseGOP: Adam Schiff‚Äôs track record:‚úîÔ∏è Spread false allegations about collusion‚úîÔ∏è Fabricated a call transcript during a congressiona‚Ä¶',\n",
       "       'Paul Krugman of @nytimes has been wrong about me from the very beginning. Anyone who has followed his ‚Äúwords of wisdom‚Äù has lost a great deal of money. Paul just concede the game say I was right and lets start a brand new game! https://t.co/O6bw61vcHL',\n",
       "       'RT @Jim_Jordan: Q: In no way shape or form... did you receive any indication whatsoever that resembled a quid pro quo. Is that correct?‚ÄùA‚Ä¶',\n",
       "       'Impeachment Witch Hunt is now OVER! Ambassador Sondland asks U.S. President (me): ‚ÄúWhat do you want from Ukraine? I keep hearing all these different ideas &amp; theories. What do you want? It was a very abrupt conversation. He was not in a good mood. He (the President) just said‚Äù...',\n",
       "       'Bob Mueller after spending two years and 45 million dollars went over all of my financials &amp; my taxes and found nothing. Now the Witch Hunt continues with local New York Democrat prosecutors going over every financial deal I have ever done. This has never happened to a.....',\n",
       "       'I never in my wildest dreams thought my name would in any way be associated with the ugly word Impeachment! The calls (Transcripts) were PERFECT there was NOTHING said that was wrong. No pressure on Ukraine. Great corruption &amp; dishonesty by Schiff on the other side!',\n",
       "       '.....President Trump said the Ukraine President should just do the right thing (No Quid Pro Quo). You shouldn‚Äôt charge but you cannot convict a sitting president on the basis of conflicting and ambiguous evidence and destabilize the American Government.‚Äù Thank you Ken!',\n",
       "       '...lawyer has already stated that I did nothing wrong. John Bolton is a patriot and may know that I held back the money from Ukraine because it is considered a corrupt country &amp; I wanted to know why nearby European countries weren‚Äôt putting up money also. Likewise I would....',\n",
       "       '.....Reserve should likewise act so that countries of which there are many no longer take advantage of our strong dollar by further devaluing their currencies. This makes it very hard for our manufactures &amp; farmers to fairly export their goods. Lower Rates &amp; Loosen - Fed!',\n",
       "       'RT @thehill: #BREAKING: Ukraine\\'s Zelensky: I never talked to Trump about \"position of a quid pro quo\" https://t.co/1oFG9XWTmj https://t.co‚Ä¶',\n",
       "       'RT @JesseBWatters: The whole thing backfired. If you look at every single swing state impeachment is now under water. #TheFive https://t.co‚Ä¶',\n",
       "       '.@NATO has now recognized SPACE as an operational domain and the alliance is STRONGER for it. U.S. leadership ensures peace through strength and we must continue to show strength and WIN on all fronts ‚Äì land air sea and SPACE!',\n",
       "       \"RT @TomFitton: Impeachment coup should be frozen until a full investigation is done on Schiff's abuse of power against @RealDonaldTrump in‚Ä¶\",\n",
       "       'RT @marklevinshow: Democrat-controlled Judiciary Committee releases more methane https://t.co/l19iQZNqa5',\n",
       "       'RT @dbongino: Ted Cruz absolutely ANNIHILATING liberal activist conspiracy theorist and fake news propagandist Chuck Todd. Chuck is the‚Ä¶',\n",
       "       '‚ÄúThe Democrats haven‚Äôt come up with a smocking gun. Nancy Pelosi by raising this to the level of Impeachment has raised the bar impossibly high. This comes after three years of nonstop investigations of Trump the Russian collusion narrative the Mueller Report &amp; now the.....',\n",
       "       'RT @CLewandowski_: Thank you @realDonaldTrump for putting American back to work.  Just think how much better it could be if Members of Cong‚Ä¶',\n",
       "       'RT @GOPChairwoman: You can‚Äôt make this up:Schiff‚Äôs Democrat counsel just flat-out refused to discuss how they conducted their partisan in‚Ä¶',\n",
       "       'RT @TeamTrump: Overflow crowd outside of the Giant Center in Hershey PA on this cold and rainy night! üëÄüá∫üá∏#TrumpRallyHershey https://t.co‚Ä¶',\n",
       "       'RT @Jim_Jordan: We thought they spied on two Americans we now know it was FOUR. The Inspector General‚Äôs report confirms what many of us‚Ä¶',\n",
       "       'RT @parscale: If the Democrat controlled House does not immediately investigate the FBI abuse found in the IG report it is clear evidence t‚Ä¶',\n",
       "       'RT @EricTrump: I will be on the @IngrahamAngle at 10pmET!!! I have no doubt it will be quite the show tonight! üá∫üá∏üá∫üá∏üá∫üá∏ #FoxNews https://t.co‚Ä¶',\n",
       "       'RT @RepDLesko: This is the most corrupt rigged railroad job I have ever seen in my entire life.',\n",
       "       \"RT @SteveScalise: WATCH ‚Üí @Jim_Jordan destroys the Dems' impeachment sham.It comes down to this:Dems never accepted the results of the‚Ä¶\",\n",
       "       'RT @dcexaminer: .@RepMattGaetz ripped the House majority\\'s impeachment counsel Daniel Goldman for donating \"tens of thousands of dollars\" t‚Ä¶',\n",
       "       'RT @Jim_Jordan: This is what it comes down to. https://t.co/lt9rMHF9dH',\n",
       "       'Congratulations to Boris Johnson on his great WIN! Britain and the United States will now be free to strike a massive new Trade Deal after BREXIT. This deal has the potential to be far bigger and more lucrative than any deal that could be made with the E.U. Celebrate Boris!',\n",
       "       'Congratulations to Tiger and the entire U.S. Team on a great comeback and tremendous WIN. True Champions! https://t.co/wyjBAgoF7J',\n",
       "       'READ THE TRANSCRIPTS! The Impeachment Hoax is the greatest con job in the history of American politics! The Fake News Media and their partner the Democrat Party are working overtime to make life for the United Republican Party and all it stands for as difficult as possible!',\n",
       "       'RT @DonaldJTrumpJr: So great to see someone willing to put party politics aside and call balls and strikes when they see them. https://t.co‚Ä¶',\n",
       "       'RT @RudyGiuliani: Evidence revealed that corruption in 2016 was so extensive it was POTUS‚Äôs DUTY to ask for US-Ukraine investigation.   I‚Ä¶',\n",
       "       'RT @RudyGiuliani: Dem‚Äôs impeachment for innocent conduct is intended to obstruct the below investigations of Obama-era corruption:- Billi‚Ä¶',\n",
       "       'The new USA Today Poll just out has me leading all of the Democrat contenders. That‚Äôs hard to believe since the Fake News &amp; 3 year Scams and Witch Hunts as phony as they are just never seem to end. The American people are smart. They see the great economy &amp; everything else!',\n",
       "       '....chance they‚Äôve had their kangaroo court they‚Äôve had their circus for weeks and months. There just isn‚Äôt anything there and there‚Äôs no way I‚Äôm voting for Impeachment for somebody who hasn‚Äôt committed any impeachable offenses.‚Äù @IngrahamAngle',\n",
       "       '....had while the Democrats are just looking out for elections. This President should just continue to fight like he‚Äôs always fought for himself &amp; for this Country. Continue to put forth policies like prescription drugs &amp; trade policies. That‚Äôs what makes this President stand..',\n",
       "       \"RT @RepArmstrongND: When you have a secret court that issues secret warrants it is inexcusable that ‚Äú[t]he FBI's submission to the court‚Ä¶\",\n",
       "       \"RT @WhiteHouse: Nancy Pelosi's do-nothing party simply can't compete with President @realDonaldTrump's record. https://t.co/mF3tNWzWhF\",\n",
       "       '‚ÄúThis is all about convicting a President based on innuendo not on the facts. Even the Ukrainian President said there was no pressure!‚Äù @RepMarkMeadows',\n",
       "       'RT @AlanDersh: Two House articles of impeachment fail to meet constitutional standards. https://t.co/jv0S3p0QzQ',\n",
       "       '....won‚Äôt convict and remove the President - Then the House should not be Impeaching the President in the first place. If this is the new standard every President from here on out is impeachable.‚Äù  Andy McCarthy @FoxNews  So well stated. Thank you!',\n",
       "       'RT @jdanbishop: 5 pgs of evidence. Endless argument. @JamesRosenTV almost alone noticed the most cogent first-hand evidence. In the 7/25 c‚Ä¶',\n",
       "       'The reason the Democrats don‚Äôt want to submit the Articles of Impeachment to the Senate is that they don‚Äôt want corrupt politician Adam Shifty Schiff to testify under oath nor do they want the Whistleblower the missing second Whistleblower the informer the Bidens to testify!',\n",
       "       'I guess the magazine ‚ÄúChristianity Today‚Äù is looking for Elizabeth Warren Bernie Sanders or those of the socialist/communist bent to guard their religion. How about Sleepy Joe? The fact is no President has ever done what I have done for Evangelicals or religion itself!',\n",
       "       'Nancy Pelosi‚Äôs District in California has rapidly become one of the worst anywhere in the U.S. when it come to the homeless &amp; crime. It has gotten so bad so fast - she has lost total control and along with her equally incompetent governor Gavin Newsom it is a very sad sight!',\n",
       "       'RT @CaliNeedsHelp: We already have the best so why look around? If you agree join us!üá±üá∑PATRIOTS UNITEüá±üá∑üî∏RT without commentingüîπReply w‚Ä¶',\n",
       "       \"RT @SJPFISH: Home Depot co-founder Slams Dem candidates for claiming Millionaires didn't work for their $$$$$$@9mmcassy@Briteeye777@pjbo‚Ä¶\",\n",
       "       'So sad to see that New York City and State are falling apart. All they want to do is investigate to make me hate them even more than I should. Governor Cuomo has lost control and lost his mind. Very bad for the homeless and all!',\n",
       "       'The anti-Semitic attack in Monsey New York on the 7th night of Hanukkah last night is horrific. We must all come together to fight confront and eradicate the evil scourge of anti-Semitism. Melania and I wish the victims a quick and full recovery.',\n",
       "       'President Putin of Russia called to thank me and the U.S. for informing them of a planned terrorist attack in the very beautiful city of Saint Petersburg. They were able to quickly apprehend the suspects with many lives being saved. Great &amp; important coordination!'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['text_clean'].str.contains('t.co')]['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:18:54.054875Z",
     "start_time": "2020-10-27T16:18:52.047543Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "## TRAINING WORD2VEC FROM FULL DF NOT JUST TARGETS\n",
    "data_lower = df['text_clean'].map(lambda x: simple_preprocess(x,True))#word_tokenize)\n",
    "\n",
    "#  data_lower= list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:19:01.218398Z",
     "start_time": "2020-10-27T16:19:01.208526Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heading',\n",
       " 'to',\n",
       " 'bank',\n",
       " 'arena',\n",
       " 'in',\n",
       " 'cincinnati',\n",
       " 'ohio',\n",
       " 'for',\n",
       " 'pm',\n",
       " 'rally',\n",
       " 'join',\n",
       " 'me',\n",
       " 'tickets']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lower[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:19:01.651516Z",
     "start_time": "2020-10-27T16:19:01.648419Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# text = simple_preprocess(' '.join(df['text']))\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:19:06.941257Z",
     "start_time": "2020-10-27T16:19:02.614996Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(data_lower,size=100,window=4,min_count=1,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:21:33.405609Z",
     "start_time": "2020-10-27T16:21:31.674188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2777847, 3507490)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(data_lower, total_examples=model.corpus_count,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:21:40.988023Z",
     "start_time": "2020-10-27T16:21:40.985679Z"
    }
   },
   "outputs": [],
   "source": [
    "wv = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:21:57.040499Z",
     "start_time": "2020-10-27T16:21:57.029682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n",
      "\n",
      "class Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n",
      " |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n",
      " |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Word2VecKeyedVectors\n",
      " |      WordEmbeddingsKeyedVectors\n",
      " |      BaseKeyedVectors\n",
      " |      gensim.utils.SaveLoad\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n",
      " |      Store the input-hidden weight matrix in the same format used by the original\n",
      " |      C word2vec-tool, for compatibility.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path used to save the vectors in\n",
      " |      fvocab : str, optional\n",
      " |          Optional file path used to save the vocabulary\n",
      " |      binary : bool, optional\n",
      " |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n",
      " |      total_vec : int, optional\n",
      " |          Optional parameter to explicitly specify total no. of vectors\n",
      " |          (in case word vectors are appended with document vectors afterwards).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  load(fname_or_handle, **kwargs) from builtins.type\n",
      " |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to file that contains needed object.\n",
      " |      mmap : str, optional\n",
      " |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n",
      " |          via mmap (shared memory) using `mmap='r'.\n",
      " |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.utils.SaveLoad.save`\n",
      " |          Save object to file.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      object\n",
      " |          Object loaded from `fname`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          When called on an object instance instead of class (this is a class method).\n",
      " |  \n",
      " |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n",
      " |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      The information stored in the file is incomplete (the binary tree is missing),\n",
      " |      so while you can query for word similarity etc., you cannot continue training\n",
      " |      with a model loaded this way.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          The file path to the saved word2vec-format file.\n",
      " |      fvocab : str, optional\n",
      " |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n",
      " |          (this is the file generated by `-save-vocab` flag of the original C tool).\n",
      " |      binary : bool, optional\n",
      " |          If True, indicates whether the data is in binary word2vec format.\n",
      " |      encoding : str, optional\n",
      " |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n",
      " |      unicode_errors : str, optional\n",
      " |          default 'strict', is a string suitable to be passed as the `errors`\n",
      " |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n",
      " |          file may include word tokens truncated in the middle of a multibyte unicode character\n",
      " |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n",
      " |      limit : int, optional\n",
      " |          Sets a maximum number of word-vectors to read from the file. The default,\n",
      " |          None, means read all.\n",
      " |      datatype : type, optional\n",
      " |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n",
      " |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n",
      " |          Loaded model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  __contains__(self, word)\n",
      " |  \n",
      " |  __init__(self, vector_size)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7ffe1e8ac598>, case_insensitive=True)\n",
      " |      Compute accuracy of the model.\n",
      " |      \n",
      " |      The accuracy is reported (=printed to log and returned as a list) for each\n",
      " |      section separately, plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      questions : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      most_similar : function, optional\n",
      " |          Function used for similarity calculation.\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of dict of (str, (str, str, str)\n",
      " |          Full lists of correct and incorrect predictions divided by sections.\n",
      " |  \n",
      " |  distance(self, w1, w2)\n",
      " |      Compute cosine distance between two words.\n",
      " |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Distance between `w1` and `w2`.\n",
      " |  \n",
      " |  distances(self, word_or_vector, other_words=())\n",
      " |      Compute cosine distances from given word or vector to all words in `other_words`.\n",
      " |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word_or_vector : {str, numpy.ndarray}\n",
      " |          Word or vector from which distances are to be computed.\n",
      " |      other_words : iterable of str\n",
      " |          For each word in `other_words` distance from `word_or_vector` is computed.\n",
      " |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.array\n",
      " |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n",
      " |      \n",
      " |      Raises\n",
      " |      -----\n",
      " |      KeyError\n",
      " |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n",
      " |  \n",
      " |  doesnt_match(self, words)\n",
      " |      Which word from the given list doesn't go with the others?\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      words : list of str\n",
      " |          List of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      str\n",
      " |          The word further away from the mean of all words.\n",
      " |  \n",
      " |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute performance of the model on an analogy test set.\n",
      " |      \n",
      " |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n",
      " |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n",
      " |      \n",
      " |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n",
      " |      plus there's one aggregate summary at the end.\n",
      " |      \n",
      " |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n",
      " |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      analogies : str\n",
      " |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n",
      " |          See `gensim/test/test_data/questions-words.txt` as example.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          The overall evaluation score on the entire evaluation set\n",
      " |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n",
      " |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n",
      " |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n",
      " |          keys 'correct' and 'incorrect'.\n",
      " |  \n",
      " |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n",
      " |      Compute correlation of the model with human similarity judgments.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      More datasets can be found at\n",
      " |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n",
      " |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pairs : str\n",
      " |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n",
      " |          See `test/test_data/wordsim353.tsv` as example.\n",
      " |      delimiter : str, optional\n",
      " |          Separator in `pairs` file.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n",
      " |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n",
      " |          in modern word embedding models).\n",
      " |      case_insensitive : bool, optional\n",
      " |          If True - convert all words to their uppercase form before evaluating the performance.\n",
      " |          Useful to handle case-mismatch between training tokens and words in the test set.\n",
      " |          In case of multiple case variants of a single word, the vector for the first occurrence\n",
      " |          (also the most frequent if vocabulary is sorted) is taken.\n",
      " |      dummy4unknown : bool, optional\n",
      " |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n",
      " |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pearson : tuple of (float, float)\n",
      " |          Pearson correlation coefficient with 2-tailed p-value.\n",
      " |      spearman : tuple of (float, float)\n",
      " |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n",
      " |          similarities produced by the model itself, with 2-tailed p-value.\n",
      " |      oov_ratio : float\n",
      " |          The ratio of pairs with unknown words.\n",
      " |  \n",
      " |  get_keras_embedding(self, train_embeddings=False, word_index=None)\n",
      " |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      train_embeddings : bool\n",
      " |          If False, the weights are frozen and stopped from being updated.\n",
      " |          If True, the weights can/will be further trained/updated.\n",
      " |      \n",
      " |      word_index : {str : int}\n",
      " |          A mapping from tokens to their indices the way they will be provided in the input to the embedding layer.\n",
      " |          The embedding of each token will be placed at the corresponding index in the returned matrix.\n",
      " |          Tokens not in the index are ignored.\n",
      " |          This is useful when the token indices are produced by a process that is not coupled with the embedding\n",
      " |          model, e.x. an Keras Tokenizer object.\n",
      " |          If None, the embedding matrix in the embedding layer will be indexed according to self.vocab\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      `keras.layers.Embedding`\n",
      " |          Embedding layer.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      Current method works only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n",
      " |  \n",
      " |  get_vector(self, word)\n",
      " |      Get the entity's representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entity : str\n",
      " |          Identifier of the entity to return the vector for.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector for the specified entity.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If the given entity identifier doesn't exist.\n",
      " |  \n",
      " |  init_sims(self, replace=False)\n",
      " |      Precompute L2-normalized vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      replace : bool, optional\n",
      " |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      You **cannot continue training** after doing a replace.\n",
      " |      The model becomes effectively read-only: you can call\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n",
      " |  \n",
      " |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      Positive words contribute positively towards the similarity, negative words negatively.\n",
      " |      \n",
      " |      This method computes cosine similarity between a simple mean of the projection\n",
      " |      weight vectors of the given words and the vectors for each word in the model.\n",
      " |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n",
      " |      word2vec implementation.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n",
      " |      Find the top-N most similar words, using the multiplicative combination objective,\n",
      " |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n",
      " |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n",
      " |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n",
      " |      In the common analogy-solving case, of two positive and one negative examples,\n",
      " |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n",
      " |      \n",
      " |      Additional positive or negative examples contribute to the numerator or denominator,\n",
      " |      respectively - a potentially sensible but untested extension of the method.\n",
      " |      With a single positive example, rankings will be the same as in the default\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      positive : list of str, optional\n",
      " |          List of words that contribute positively.\n",
      " |      negative : list of str, optional\n",
      " |          List of words that contribute negatively.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  n_similarity(self, ws1, ws2)\n",
      " |      Compute cosine similarity between two sets of words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ws1 : list of str\n",
      " |          Sequence of words.\n",
      " |      ws2: list of str\n",
      " |          Sequence of words.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Similarities between `ws1` and `ws2`.\n",
      " |  \n",
      " |  relative_cosine_similarity(self, wa, wb, topn=10)\n",
      " |      Compute the relative cosine similarity between two words given top-n similar words,\n",
      " |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n",
      " |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n",
      " |      \n",
      " |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n",
      " |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n",
      " |      any arbitrary word pairs.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      wa: str\n",
      " |          Word for which we have to look top-n similar word.\n",
      " |      wb: str\n",
      " |          Word for which we evaluating relative cosine similarity with wa.\n",
      " |      topn: int, optional\n",
      " |          Number of top-n similar words to look with respect to wa.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.float64\n",
      " |          Relative cosine similarity between wa and wb.\n",
      " |  \n",
      " |  save(self, *args, **kwargs)\n",
      " |      Save KeyedVectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : str\n",
      " |          Path to the output file.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n",
      " |          Load saved model.\n",
      " |  \n",
      " |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words by vector.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector : numpy.array\n",
      " |          Vector from which similarities are to be computed.\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n",
      " |          then similarities for all words are returned.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n",
      " |      Find the top-N most similar words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Word\n",
      " |      topn : int or None, optional\n",
      " |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n",
      " |          the vector of similarity scores.\n",
      " |      restrict_vocab : int, optional\n",
      " |          Optional integer which limits the range of vectors which\n",
      " |          are searched for most-similar values. For example, restrict_vocab=10000 would\n",
      " |          only check the first 10000 word vectors in the vocabulary order. (This may be\n",
      " |          meaningful if you've sorted the vocabulary by descending frequency.)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list of (str, float) or numpy.array\n",
      " |          When `topn` is int, a sequence of (word, similarity) is returned.\n",
      " |          When `topn` is None, then similarities for all words are returned as a\n",
      " |          one-dimensional numpy array with the size of the vocabulary.\n",
      " |  \n",
      " |  similarity(self, w1, w2)\n",
      " |      Compute cosine similarity between two words.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Cosine similarity between `w1` and `w2`.\n",
      " |  \n",
      " |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n",
      " |      Construct a term similarity matrix for computing Soft Cosine Measure.\n",
      " |      \n",
      " |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n",
      " |      Soft Cosine Measure between documents.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n",
      " |          A dictionary that specifies the considered terms.\n",
      " |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n",
      " |          A model that specifies the relative importance of the terms in the dictionary. The\n",
      " |          columns of the term similarity matrix will be build in a decreasing order of importance\n",
      " |          of terms, or in the order of term identifiers if None.\n",
      " |      threshold : float, optional\n",
      " |          Only embeddings more similar than `threshold` are considered when retrieving word\n",
      " |          embeddings closest to a given word embedding.\n",
      " |      exponent : float, optional\n",
      " |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n",
      " |      nonzero_limit : int, optional\n",
      " |          The maximum number of non-zero elements outside the diagonal in a single column of the\n",
      " |          sparse term similarity matrix.\n",
      " |      dtype : numpy.dtype, optional\n",
      " |          Data-type of the sparse term similarity matrix.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      :class:`scipy.sparse.csc_matrix`\n",
      " |          Term similarity matrix.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      :func:`gensim.matutils.softcossim`\n",
      " |          The Soft Cosine Measure.\n",
      " |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n",
      " |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n",
      " |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n",
      " |      between Questions for Community Question Answering\", 2017\n",
      " |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n",
      " |  \n",
      " |  wmdistance(self, document1, document2)\n",
      " |      Compute the Word Mover's Distance between two documents.\n",
      " |      \n",
      " |      When using this code, please consider citing the following papers:\n",
      " |      \n",
      " |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n",
      " |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n",
      " |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n",
      " |        <https://ieeexplore.ieee.org/document/5459199/>`_\n",
      " |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n",
      " |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      document1 : list of str\n",
      " |          Input document.\n",
      " |      document2 : list of str\n",
      " |          Input document.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      float\n",
      " |          Word Mover's distance between `document1` and `document2`.\n",
      " |      \n",
      " |      Warnings\n",
      " |      --------\n",
      " |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n",
      " |      \n",
      " |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n",
      " |      will be returned.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      ImportError\n",
      " |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n",
      " |  \n",
      " |  word_vec(self, word, use_norm=False)\n",
      " |      Get `word` representations in vector space, as a 1D numpy array.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      word : str\n",
      " |          Input word\n",
      " |      use_norm : bool, optional\n",
      " |          If True - resulting vector will be L2-normalized (unit euclidean length).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation of `word`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      KeyError\n",
      " |          If word not in vocabulary.\n",
      " |  \n",
      " |  words_closer_than(self, w1, w2)\n",
      " |      Get all words that are closer to `w1` than `w2` is to `w1`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      w1 : str\n",
      " |          Input word.\n",
      " |      w2 : str\n",
      " |          Input word.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      list (str)\n",
      " |          List of words that are closer to `w1` than `w2` is to `w1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  cosine_similarities(vector_1, vectors_all)\n",
      " |      Compute cosine similarities between one vector and a set of other vectors.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      vector_1 : numpy.ndarray\n",
      " |          Vector from which similarities are to be computed, expected shape (dim,).\n",
      " |      vectors_all : numpy.ndarray\n",
      " |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n",
      " |  \n",
      " |  log_accuracy(section)\n",
      " |  \n",
      " |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n",
      " |  \n",
      " |  index2entity\n",
      " |  \n",
      " |  syn0\n",
      " |  \n",
      " |  syn0norm\n",
      " |  \n",
      " |  wv\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseKeyedVectors:\n",
      " |  \n",
      " |  __getitem__(self, entities)\n",
      " |      Get vector representation of `entities`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Input entity/entities.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      numpy.ndarray\n",
      " |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n",
      " |  \n",
      " |  __setitem__(self, entities, weights)\n",
      " |      Add entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n",
      " |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : {str, list of str}\n",
      " |          Entities specified by their string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or 2D np.array of vectors.\n",
      " |  \n",
      " |  add(self, entities, weights, replace=False)\n",
      " |      Append entities and theirs vectors in a manual way.\n",
      " |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      entities : list of str\n",
      " |          Entities specified by string ids.\n",
      " |      weights: list of numpy.ndarray or numpy.ndarray\n",
      " |          List of 1D np.array vectors or a 2D np.array of vectors.\n",
      " |      replace: bool, optional\n",
      " |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n",
      " |          if True - replace vectors, otherwise - keep old vectors.\n",
      " |  \n",
      " |  closer_than(self, entity1, entity2)\n",
      " |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n",
      " |  \n",
      " |  most_similar_to_given(self, entity1, entities_list)\n",
      " |      Get the `entity` from `entities_list` most similar to `entity1`.\n",
      " |  \n",
      " |  rank(self, entity1, entity2)\n",
      " |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from gensim.utils.SaveLoad:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:22:21.339530Z",
     "start_time": "2020-10-27T16:22:21.334711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.97725177e-01, -1.02185011e+00, -1.69189334e-01, -1.37412810e+00,\n",
       "        1.47666514e+00, -3.74030292e-01,  2.40638542e+00,  1.76545548e+00,\n",
       "       -9.86887336e-01, -1.50291061e+00, -1.17016053e+00, -1.30776596e+00,\n",
       "        8.20626020e-02,  1.87480831e+00,  4.31128830e-01,  4.67176348e-01,\n",
       "       -1.17880784e-01,  2.19573164e+00,  1.59704947e+00, -1.32507336e+00,\n",
       "        1.30375671e+00, -8.85268748e-01,  7.43142426e-01, -1.30962408e+00,\n",
       "        6.65495172e-02,  5.80909431e-01, -2.88448781e-01,  4.66280580e-01,\n",
       "       -2.40665030e+00, -1.72075272e-01,  7.96539605e-01, -3.48868757e-01,\n",
       "       -1.45647514e+00, -8.85340095e-01,  1.95581961e+00,  6.16921604e-01,\n",
       "       -1.81026196e+00, -1.85274065e+00,  1.30017591e+00, -1.50146902e-01,\n",
       "       -1.31920803e+00, -1.49311972e+00,  5.02608232e-02, -1.39201117e+00,\n",
       "       -2.47322046e-03, -2.25241327e+00,  5.77637494e-01,  7.56581903e-01,\n",
       "       -4.66473192e-01,  1.29378021e+00, -8.13583314e-01, -5.35659850e-01,\n",
       "       -2.22131801e+00, -6.07964933e-01,  7.52435744e-01,  2.56357455e+00,\n",
       "       -1.18853700e+00,  7.20044792e-01,  9.79643226e-01, -5.32226026e-01,\n",
       "       -8.23052943e-01, -6.21642351e-01, -1.45176661e+00,  5.85703313e-01,\n",
       "        1.21412778e+00, -1.13562262e+00,  1.97173941e+00,  1.60790944e+00,\n",
       "       -7.41132379e-01, -9.46522474e-01, -1.62585235e+00,  7.18716443e-01,\n",
       "        8.92012715e-01,  8.51233378e-02, -1.15179062e+00,  2.61683643e-01,\n",
       "       -1.26768267e+00,  1.19537532e+00,  2.07649469e+00,  2.09721398e+00,\n",
       "       -1.11385798e+00, -2.54610777e+00,  8.89245033e-01,  1.44102097e-01,\n",
       "       -1.98427665e+00, -2.00756863e-01, -7.53584981e-01,  4.62857395e-01,\n",
       "       -1.63862780e-01,  7.00784385e-01, -6.25240505e-01, -1.16645050e+00,\n",
       "        1.05478615e-01, -7.19307438e-02, -8.79421830e-01, -2.07026795e-01,\n",
       "        5.37204385e-01, -1.62820935e+00,  1.78176069e+00, -1.15563405e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv['republican']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:22:45.739041Z",
     "start_time": "2020-10-27T16:22:45.722574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('democrat', 0.7401123642921448),\n",
       " ('senators', 0.7057304382324219),\n",
       " ('party', 0.7018539905548096),\n",
       " ('democratic', 0.6777942776679993),\n",
       " ('candidates', 0.6435932517051697),\n",
       " ('thedemocrats', 0.6297471523284912),\n",
       " ('opposition', 0.6088434457778931),\n",
       " ('bitterness', 0.6044570803642273),\n",
       " ('senjohnkennedy', 0.595993161201477),\n",
       " ('republicans', 0.5892543792724609)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('republican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:23:27.950904Z",
     "start_time": "2020-10-27T16:23:27.943523Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lifeline', 0.4959105849266052),\n",
       " ('skorea', 0.4835852384567261),\n",
       " ('nate', 0.4791489541530609),\n",
       " ('breakthru', 0.4329405725002289),\n",
       " ('headway', 0.38289856910705566),\n",
       " ('survive', 0.3765192925930023),\n",
       " ('rexnord', 0.3763708770275116),\n",
       " ('charismatic', 0.37339574098587036),\n",
       " ('proposing', 0.368845134973526),\n",
       " ('japan', 0.3668920397758484)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar(negative=['democrat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "## For initializing model\n",
    "sentences=None,\n",
    "    size=100,\n",
    "    alpha=0.025,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    max_vocab_size=None,\n",
    "    sample=0.001,\n",
    "    seed=1,\n",
    "    workers=3,\n",
    "    min_alpha=0.0001,\n",
    "    sg=0,\n",
    "    hs=0,\n",
    "    negative=5,\n",
    "    cbow_mean=1,\n",
    "    hashfxn=<built-in function hash>,\n",
    "    iter=5,\n",
    "    null_word=0,\n",
    "    trim_rule=None,\n",
    "    sorted_vocab=1,\n",
    "    batch_words=10000,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    \n",
    "    \n",
    "## For training \n",
    "    sentences,\n",
    "    total_examples=None,\n",
    "    total_words=None,\n",
    "    epochs=None,\n",
    "    start_alpha=None,\n",
    "    end_alpha=None,\n",
    "    word_count=0,\n",
    "    queue_factor=2,\n",
    "    report_delay=1.0,\n",
    "    compute_loss=False,\n",
    "    callbacks=(),\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:24:23.602270Z",
     "start_time": "2020-10-27T16:24:23.596406Z"
    }
   },
   "outputs": [],
   "source": [
    "# ### USING WORD VECTOR MATH TO GET A FEEL FOR QUALITY OF MODE\n",
    "def word_math(wv,pos_words=['hillary'],neg_words=['bill'],\n",
    "              verbose=True,return_vec=False):\n",
    "    if isinstance(pos_words,str):\n",
    "        pos_words=[pos_words]\n",
    "    if isinstance(neg_words,str):\n",
    "        neg_words=[neg_words]\n",
    "\n",
    "\n",
    "    pos_eqn = '+'.join(pos_words)\n",
    "    neg_eqn = '-'.join(neg_words)\n",
    "\n",
    "    print('---'*15)    \n",
    "    print(f\"[i] Result for:\\t{pos_eqn}{' - '+neg_eqn if len(neg_eqn)>0 else ' '}\")\n",
    "    print('---'*15)\n",
    "\n",
    "    answer = wv.most_similar(positive=pos_words,negative=neg_words)\n",
    "    \n",
    "    if verbose:\n",
    "          [print(f\"- {ans[0]} ({round(ans[1],3)})\") for ans in answer]\n",
    "          print('---'*15,'\\n\\n')\n",
    "\n",
    "    if return_vec: \n",
    "          return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T16:24:34.895682Z",
     "start_time": "2020-10-27T16:24:34.878533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "[i] Result for:\tamerica+crime \n",
      "---------------------------------------------\n",
      "- borders (0.736)\n",
      "- strong (0.652)\n",
      "- choice (0.625)\n",
      "- trajectory (0.622)\n",
      "- tough (0.6)\n",
      "- bracing (0.598)\n",
      "- ussgeraldrford (0.597)\n",
      "- agenda (0.594)\n",
      "- vamissionact (0.589)\n",
      "- causing (0.583)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tdemocrats+russia \n",
      "---------------------------------------------\n",
      "- dems (0.752)\n",
      "- facts (0.732)\n",
      "- hoax (0.716)\n",
      "- russian (0.711)\n",
      "- russians (0.687)\n",
      "- dnc (0.674)\n",
      "- phony (0.66)\n",
      "- collusion (0.658)\n",
      "- questions (0.657)\n",
      "- rigged (0.655)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trepublican - honor\n",
      "---------------------------------------------\n",
      "- democrat (0.658)\n",
      "- dem (0.563)\n",
      "- dems (0.527)\n",
      "- dre (0.51)\n",
      "- republicans (0.497)\n",
      "- democratic (0.46)\n",
      "- liberal (0.454)\n",
      "- veered (0.443)\n",
      "- relying (0.442)\n",
      "- quitters (0.439)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tman+power \n",
      "---------------------------------------------\n",
      "- person (0.723)\n",
      "- hatred (0.669)\n",
      "- thief (0.667)\n",
      "- leader (0.638)\n",
      "- connecticut (0.636)\n",
      "- wilson (0.615)\n",
      "- worn (0.614)\n",
      "- wisdom (0.613)\n",
      "- pled (0.611)\n",
      "- language (0.609)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\trussia+honor \n",
      "---------------------------------------------\n",
      "- privilege (0.714)\n",
      "- perfect (0.683)\n",
      "- call (0.663)\n",
      "- relations (0.657)\n",
      "- definition (0.653)\n",
      "- poroshenko (0.652)\n",
      "- campaign (0.652)\n",
      "- russians (0.645)\n",
      "- ukraine (0.641)\n",
      "- salary (0.638)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n",
      "---------------------------------------------\n",
      "[i] Result for:\tchina - tariff\n",
      "---------------------------------------------\n",
      "- president (0.4)\n",
      "- stifled (0.392)\n",
      "- emanuel (0.365)\n",
      "- chariman (0.357)\n",
      "- rooted (0.352)\n",
      "- administration (0.334)\n",
      "- putin (0.334)\n",
      "- relationship (0.332)\n",
      "- lifeline (0.331)\n",
      "- adminis (0.321)\n",
      "--------------------------------------------- \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "equation_list=[(['america','crime'],[]),\n",
    "               \n",
    "               (['democrats','russia'],[]),\n",
    "               (['republican'],['honor']),\n",
    "               (['man','power'],[]),\n",
    "               (['russia','honor'],[]),\n",
    "              (['china','tariff'])]\n",
    "\n",
    "for eqn in equation_list:\n",
    "#     print('\\n\\n')\n",
    "    word_math(wv,*eqn)\n",
    "#     word_math(wv2,*eqn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe - Global Vectors for Word Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Usually embeddings are hundreds of dimensions\n",
    "- Just use the word embeddings already learned from before!\n",
    "    + Unless very specific terminology, context will likely carry within language\n",
    "- Comparable to CNN transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Models - ANNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/rnn.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-understanding-recurrent-neural-networks-online-ds-ft-100719/master/images/unrolled.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/jirvingphd/dsc-sequence-model-use-cases-online-ds-ft-100719/master/images/RNN-unrolled.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs GRUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRU (Gated Recurrent Units (GRUs)\n",
    "    - Reset Gate\n",
    "    - Update Gate\n",
    "    \n",
    "- LSTM (Long Short Term Memory Cells)\n",
    "   - Input Gate\n",
    "   - Forget Gate\n",
    "   - Output Gate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings in Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Layers\n",
    "You should make note of a couple caveats that come with using embedding layers in your neural network -- namely:\n",
    "\n",
    "* The embedding layer must always be the first layer of the network, meaning that it should immediately follow the `Input()` layer \n",
    "* All words in the text should be integer-encoded, with each unique word encoded as it's own unique integer  \n",
    "* The size of the embedding layer must always be greater than the total vocabulary size of the dataset! The first parameter denotes the vocabulary size, while the second denotes the size of the actual word vectors\n",
    "* The size of the sequences passed in as data must be set when creating the layer (all data will be converted to padded sequences of the same size during the preprocessing step) \n",
    "\n",
    "\n",
    "[Keras Documentation for Embedding Layers](https://keras.io/layers/embeddings/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pre-Trained Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:23:35.162833Z",
     "start_time": "2020-10-27T15:23:35.158544Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '/Users/jamesirving/Datasets/'#glove.twitter.27B/'\n",
    "# print(os.listdir(folder))\n",
    "glove_file = folder+'glove.6B/glove.6B.50d.txt'#'glove.twitter.27B.50d.txt'\n",
    "glove_twitter_file = folder+'glove.twitter.27B/glove.twitter.27B.50d.txt'\n",
    "print(glove_file)\n",
    "print(glove_twitter_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keeping only the vectors needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:23:35.654892Z",
     "start_time": "2020-10-27T15:23:35.617305Z"
    }
   },
   "outputs": [],
   "source": [
    "## This line of code for getting all words bugs me\n",
    "total_vocabulary = set(word for tweet in data_lower for word in tweet)\n",
    "len(total_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:23:36.995769Z",
     "start_time": "2020-10-27T15:23:35.867728Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open(glove_file,'rb') as f:#'glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:23:37.002033Z",
     "start_time": "2020-10-27T15:23:36.997684Z"
    }
   },
   "outputs": [],
   "source": [
    "glove['republican']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embeddings in Classification Models - sci-kit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Embeddings can be used in Artificial Neural Networks as an input Embedding Layer\n",
    "- Embeddings can be used in sci-kit learn models by taking the mean vector of a text/document and using the mean vector as the input into the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Finding Trump Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the beginning of his presidency, Trump continued to use a non-secure Android phone for his personal use.\n",
    "- Tweets from this period can be attributed to Trump or his staffers based on if it came from an Android device or an iPhone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:28.088051Z",
     "start_time": "2020-10-27T15:24:26.976738Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../trump_tweets_12012016_to_01012020.csv\",\n",
    "                parse_dates=['created_at'],index_col='created_at')#'https://raw.githubusercontent.com/jirvingphd/capstone-project-using-trumps-tweets-to-predict-stock-market/master/data/trump_tweets_12012016_to_01012020.csv')\n",
    "# df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df = df.sort_index()#.set_index('datetime')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:28.975597Z",
     "start_time": "2020-10-27T15:24:28.966288Z"
    }
   },
   "outputs": [],
   "source": [
    "devices = ['Twitter for Android','Twitter for iPhone']\n",
    "print(f'The first and last timestamps for the group {devices[0]}are:')\n",
    "start,end= df.groupby('source').get_group(devices[0]).index[[0,-1]]\n",
    "print(start) ,print(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:29.766222Z",
     "start_time": "2020-10-27T15:24:29.753444Z"
    }
   },
   "outputs": [],
   "source": [
    "## Puttig it all together\n",
    "df_data = df[df['source'].isin(devices)].loc[start:end].copy()\n",
    "df_data['source'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:30.398224Z",
     "start_time": "2020-10-27T15:24:30.380942Z"
    }
   },
   "outputs": [],
   "source": [
    "undersample = False\n",
    "\n",
    "if undersample:\n",
    "    ## Undersampling to match class\n",
    "    iphone = df_data.loc[df_data['source']=='Twitter for iPhone']\n",
    "    android = df_data.loc[df_data['source']=='Twitter for Android']\n",
    "    print(len(iphone),len(android))\n",
    "    df = pd.concat([iphone, \n",
    "                    android.sample(n=len(iphone),\n",
    "                                   random_state=123)],\n",
    "                    axis=0)\n",
    "    print('Data has been undersampled to match mintority class.')\n",
    "else:\n",
    "    print('No resampling was done.')\n",
    "    df= df_data\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:31.147301Z",
     "start_time": "2020-10-27T15:24:31.145347Z"
    }
   },
   "outputs": [],
   "source": [
    "# # df.to_csv('datasets/trump_tweets_iphone_vs_twitter.csv',index=False)\n",
    "# df= pd.read_csv('datasets/trump_tweets_iphone_vs_twitter.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:31.461468Z",
     "start_time": "2020-10-27T15:24:31.456372Z"
    }
   },
   "outputs": [],
   "source": [
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Mean Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:35.430416Z",
     "start_time": "2020-10-27T15:24:35.264159Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "\n",
    "y = pd.get_dummies(df['source'],drop_first=True).values\n",
    "X = df['text'].str.lower().map(word_tokenize)\n",
    "\n",
    "X_idx = list(range(len(X)))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "X[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:37.013471Z",
     "start_time": "2020-10-27T15:24:37.008986Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split_idx(X,y,train_idx,test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:37.653557Z",
     "start_time": "2020-10-27T15:24:37.651510Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['combined_text'] = df['headline'] + ' ' + df['short_description']\n",
    "# data = df['combined_text'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:38.352243Z",
     "start_time": "2020-10-27T15:24:38.347110Z"
    }
   },
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:39.109028Z",
     "start_time": "2020-10-27T15:24:39.107212Z"
    }
   },
   "outputs": [],
   "source": [
    "# target = df['source']\n",
    "# data = df['text'].map(word_tokenize)\n",
    "# data_lower = list(map(lambda x: [w.lower() for w in x],data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:39.582238Z",
     "start_time": "2020-10-27T15:24:39.445625Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:40.659887Z",
     "start_time": "2020-10-27T15:24:40.119553Z"
    }
   },
   "outputs": [],
   "source": [
    "scores = [(name, cross_val_score(model, X_train, y_train, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:40.805020Z",
     "start_time": "2020-10-27T15:24:40.801585Z"
    }
   },
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Embedding Layers in ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:47.241750Z",
     "start_time": "2020-10-27T15:24:41.687432Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, LSTM, Embedding\n",
    "from keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:47.249787Z",
     "start_time": "2020-10-27T15:24:47.243069Z"
    }
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "sys.path.append(\"../\")\n",
    "import keras_gridsearch as kg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:47.256001Z",
     "start_time": "2020-10-27T15:24:47.251972Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import text,sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn import metrics\n",
    "\n",
    "# y_t = pd.get_dummies(df['source'])#.values\n",
    "# y_t=y_t['Twitter for Android'].values\n",
    "y_t = to_categorical(df['source'].map({'Twitter for Android':1,\n",
    "                                       'Twitter for iPhone':0}),num_classes=2)\n",
    "\n",
    "# y_t.name='True Trump'\n",
    "\n",
    "# print(y_t.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:47.287278Z",
     "start_time": "2020-10-27T15:24:47.257825Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 25000\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=MAX_WORDS)\n",
    "\n",
    "tokenizer.fit_on_texts(X) #df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(X) #df['text'])\n",
    "\n",
    "X_t = sequence.pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:47.292912Z",
     "start_time": "2020-10-27T15:24:47.288931Z"
    }
   },
   "outputs": [],
   "source": [
    "X_t[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:47.299719Z",
     "start_time": "2020-10-27T15:24:47.294684Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test =train_test_split(X_t,y_t,random_state=123) \n",
    "X_train.shape,y_test.shape\n",
    "# pd.Series(y_test).value_counts(normalize=True)\n",
    "y_test.shape\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:47.319254Z",
     "start_time": "2020-10-27T15:24:47.301449Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "\n",
    "embedding_matrix = np.zeros((len(total_vocabulary) + 1, EMBEDDING_SIZE))\n",
    "for word, i in enumerate(total_vocabulary):#.items():\n",
    "    embedding_vector = glove.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "embedding_layer = Embedding(len(total_vocabulary) + 1,\n",
    "                            EMBEDDING_SIZE,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:53.070678Z",
     "start_time": "2020-10-27T15:24:47.322255Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_SIZE = 128 #where codealong get this?\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "model.add(LSTM(25,return_sequences=True))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(25, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',#'categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = model.predict_classes(X_test)\n",
    "# print(pd.Series(y_hat_test).value_counts())\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:53.074154Z",
     "start_time": "2020-10-27T15:24:53.072432Z"
    }
   },
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "# model.add(LSTM(output_dim=256, activation='sigmoid', recurrent_activation='hard_sigmoid', return_sequences=True))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(LSTM(output_dim=256, activation='sigmoid', recurrent_activation='hard_sigmoid'))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:53.080252Z",
     "start_time": "2020-10-27T15:24:53.077164Z"
    }
   },
   "outputs": [],
   "source": [
    "y_hat_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-17T00:18:01.708621Z",
     "start_time": "2020-02-17T00:18:00.772373Z"
    }
   },
   "source": [
    "## RNN or GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:24:53.478265Z",
     "start_time": "2020-10-27T15:24:53.081867Z"
    }
   },
   "outputs": [],
   "source": [
    "## GRU Model\n",
    "from keras import models, layers, optimizers, regularizers\n",
    "modelG = models.Sequential()\n",
    "\n",
    "## Get and add embedding_layer\n",
    "# embedding_layer = ji.make_keras_embedding_layer(wv, X_train)\n",
    "modelG.add(Embedding(MAX_WORDS, EMBEDDING_SIZE))\n",
    "\n",
    "# modelG.add(layers.SpatialDropout1D(0.5))\n",
    "# modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2,return_sequences=True)))\n",
    "modelG.add(layers.Bidirectional(layers.GRU(units=100, dropout=0.5, recurrent_dropout=0.2)))\n",
    "modelG.add(layers.Dense(2, activation='softmax'))\n",
    "\n",
    "modelG.compile(loss='categorical_crossentropy',optimizer=\"adam\",metrics=['acc'])#,'val_acc'])#, callbacks=callbacks)\n",
    "modelG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T15:25:10.666057Z",
     "start_time": "2020-10-27T15:24:53.480639Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "history = modelG.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "y_hat_test = modelG.predict_classes(X_test)\n",
    "kg.evaluate_model(y_test,y_hat_test,history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn vectorization\n",
    "WIP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.064879Z",
     "start_time": "2020-10-24T19:54:27.061895Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "tf_transformer = TfidfTransformer(use_idf=False)#TfidfTransformer()\n",
    "tfidf_transformer = TfidfTransformer(use_idf=True)#TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.071273Z",
     "start_time": "2020-10-24T19:54:27.067418Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le= LabelEncoder()\n",
    "y = le.fit_transform(df['source'])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.092296Z",
     "start_time": "2020-10-24T19:54:27.073033Z"
    }
   },
   "outputs": [],
   "source": [
    "X = count_vectorizer.fit_transform(df['text'])\n",
    "X_tf = tf_transformer.fit_transform(X)\n",
    "X_tfidf = tfidf_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.097369Z",
     "start_time": "2020-10-24T19:54:27.094153Z"
    }
   },
   "outputs": [],
   "source": [
    "X_tf.shape,X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.103938Z",
     "start_time": "2020-10-24T19:54:27.098968Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_idx = list(range(X.shape[0]))\n",
    "train_idx,test_idx = train_test_split(X_idx,random_state=123)\n",
    "\n",
    "\n",
    "def train_test_split_idx(X, y, train_idx,test_idx):\n",
    "    # try count vectorized first\n",
    "    X_train = X[train_idx].copy()\n",
    "    y_train = y[train_idx].copy()\n",
    "    X_test = X[train_idx].copy()\n",
    "    y_test = y[train_idx].copy()\n",
    "    return X_train, X_test,y_train, y_test\n",
    "\n",
    "X_dict = {'count':X_tf,\n",
    "         'tfidf':X_tfidf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.107601Z",
     "start_time": "2020-10-24T19:54:27.105443Z"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "\n",
    "# svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#                 ('Support Vector Machine', SVC())])\n",
    "\n",
    "# lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Logistic Regression', LogisticRegression())])\n",
    "\n",
    "# models = {'Random Forest':RandomForestClassifier(n_estimators=100, verbose=True),\n",
    "#           'SVC':SVC(),'lr':LogisticRegression()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.111041Z",
     "start_time": "2020-10-24T19:54:27.109183Z"
    }
   },
   "outputs": [],
   "source": [
    "# res = [['Method','Model',\"Result\"]]\n",
    "\n",
    "# for tf_type,X_data in X_dict.items():\n",
    "#     X_train, X_test,y_train, y_test = train_test_split_idx(X_data,y,train_idx,test_idx)\n",
    "    \n",
    "#     for name, model in models.items():\n",
    "    \n",
    "# #     rf = RandomForestClassifier(n_estimators=100,verbose=True)\n",
    "#         cv_res = cross_val_score(model, X_train,y_train, cv=5)\n",
    "#         res.append([tf_type,name,cv_res.mean()])\n",
    "\n",
    "# pd.DataFrame(res[1:],columns=res[0]).sort_values(\"Result\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.114475Z",
     "start_time": "2020-10-24T19:54:27.112575Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# rf_params = dict(n_estimators=100, verbose=True)\n",
    "\n",
    "# rf =Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Random Forest',RandomForestClassifier(**rf_params))])\n",
    "\n",
    "# svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#                 ('Support Vector Machine', SVC())])\n",
    "\n",
    "# lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(glove)),\n",
    "#               ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.119280Z",
     "start_time": "2020-10-24T19:54:27.117492Z"
    }
   },
   "outputs": [],
   "source": [
    "# models = [('Random Forest', rf),\n",
    "#           ('Support Vector Machine', svc),\n",
    "#           ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:54:27.122880Z",
     "start_time": "2020-10-24T19:54:27.120989Z"
    }
   },
   "outputs": [],
   "source": [
    "# # res = [['Model','Score']]\n",
    "# res=[['Model','Scores']]\n",
    "# for (name, model) in models:\n",
    "#     print(name)\n",
    "#     cv_res = cross_val_score(model, data_lower, df['source'], cv=5).mean()\n",
    "#     res.append([name,cv_res])\n",
    "    \n",
    "# pd.DataFrame(res[1:],columns=res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practicing Text Preprocessing with Trump's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LAST CLASS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.118668Z",
     "start_time": "2020-10-24T19:48:22.638Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a list of stopwords to remove\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.119643Z",
     "start_time": "2020-10-24T19:48:22.642Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get all the stop words in the English language\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list+=string.punctuation\n",
    "print(stopwords_list)\n",
    "stopwords_list.remove('until')\n",
    "stopwords_list.extend(['‚Äú','...','‚Äù'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.120623Z",
     "start_time": "2020-10-24T19:48:22.645Z"
    }
   },
   "outputs": [],
   "source": [
    "## Commentary on not always accepting what is or isn't in stopwords\n",
    "'until' in stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.121511Z",
     "start_time": "2020-10-24T19:48:22.649Z"
    }
   },
   "outputs": [],
   "source": [
    "stopped_tokens = [w.lower() for w in tokens if w.lower() not in stopwords_list]\n",
    "freq = FreqDist(stopped_tokens)\n",
    "freq.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.122476Z",
     "start_time": "2020-10-24T19:48:22.652Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.123468Z",
     "start_time": "2020-10-24T19:48:22.656Z"
    }
   },
   "outputs": [],
   "source": [
    "## Get FreqDist for Cleaned Text Data\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Phases of Proprocessing/Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.124373Z",
     "start_time": "2020-10-24T19:48:22.660Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# def clean_text(text,exclude_words=['until']):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "#     ## tokenize text\n",
    "#     tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.125331Z",
     "start_time": "2020-10-24T19:48:22.663Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from ipywidgets import interact\n",
    "\n",
    "@interact\n",
    "def tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "    \n",
    "    print(f\"- Tweet #{i}:\\n\")\n",
    "    print(corpus[i],'\\n')\n",
    "    tokens = word_tokenize(corpus[i])\n",
    "\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    \n",
    "    print(tokens,end='\\n\\n')\n",
    "    print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best regexp resource and tester: https://regex101.com/\n",
    "\n",
    "    - Make sure to check \"Python\" under Flavor menu on left side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.126252Z",
     "start_time": "2020-10-24T19:48:22.668Z"
    }
   },
   "outputs": [],
   "source": [
    "text =  corpus[6615]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.127253Z",
     "start_time": "2020-10-24T19:48:22.671Z"
    }
   },
   "outputs": [],
   "source": [
    "text2=corpus[7347]\n",
    "text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.128126Z",
     "start_time": "2020-10-24T19:48:22.675Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk import regexp_tokenize\n",
    "pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.129095Z",
     "start_time": "2020-10-24T19:48:22.678Z"
    }
   },
   "outputs": [],
   "source": [
    "print('[i] Word Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print('\\n[i] Regexp Tokenize:',end='\\n'+'---'*20+'\\n')\n",
    "print(regexp_tokenize(text,pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.130037Z",
     "start_time": "2020-10-24T19:48:22.681Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text,regex=True):\n",
    "    from nltk.corpus import stopwords\n",
    "    import string\n",
    "    from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "    ## tokenize text\n",
    "    if regex:\n",
    "        pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "        tokens= regexp_tokenize(text,pattern)\n",
    "    else:\n",
    "        tokens = word_tokenize(text)\n",
    "    # Get all the stop words in the English language\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list += string.punctuation\n",
    "    stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "    return stopped_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.131047Z",
     "start_time": "2020-10-24T19:48:22.685Z"
    }
   },
   "outputs": [],
   "source": [
    "# @interact\n",
    "# def regexp_tokenize_tweet(i=(0,len(corpus)-1)):\n",
    "#     print(f\"- Tweet #{i}:\\n\")\n",
    "#     print(corpus[i],'\\n')\n",
    "#     from nltk import regexp_tokenize\n",
    "#     pattern = \"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#     tokens= regexp_tokenize(corpus[i],pattern)\n",
    "\n",
    "#     # It is usually a good idea to lowercase all tokens during this step, as well\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     print(tokens,end='\\n\\n')\n",
    "#     return print(stopped_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.131905Z",
     "start_time": "2020-10-24T19:48:22.689Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def find_urls(string): \n",
    "    return re.findall(r\"(http[s]?://\\w*\\.\\w*/+\\w+)\",string)\n",
    "\n",
    "def find_hashtags(string):\n",
    "    return re.findall(r'\\#\\w*',string)\n",
    "\n",
    "def find_retweets(string):\n",
    "    return re.findall(r'RT [@]?\\w*:',string)\n",
    "\n",
    "def find_mentions(string):\n",
    "    return re.findall(r'\\@\\w*',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.132878Z",
     "start_time": "2020-10-24T19:48:22.692Z"
    }
   },
   "outputs": [],
   "source": [
    "find_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.133833Z",
     "start_time": "2020-10-24T19:48:22.695Z"
    }
   },
   "outputs": [],
   "source": [
    "find_mentions(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.134884Z",
     "start_time": "2020-10-24T19:48:22.700Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('feet')) # foot\n",
    "print(lemmatizer.lemmatize('running')) # run [?!] Does not match expected output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.135854Z",
     "start_time": "2020-10-24T19:48:22.704Z"
    }
   },
   "outputs": [],
   "source": [
    "text_in =  corpus[6615]\n",
    "\n",
    "# # urls = find_urls(text)\n",
    "# def clean_text(text,regex=True):\n",
    "#     from nltk.corpus import stopwords\n",
    "#     import string\n",
    "#     from nltk import word_tokenize,regexp_tokenize\n",
    "\n",
    "#     ## tokenize text\n",
    "#     if regex:\n",
    "#         pattern = r\"([a-zA-Z]+(?:'[a-z]+)?)\"\n",
    "#         tokens= regexp_tokenize(text,pattern)\n",
    "#     else:\n",
    "#         tokens = word_tokenize(text)\n",
    "#     # Get all the stop words in the English language\n",
    "#     stopwords_list = stopwords.words('english')\n",
    "#     stopwords_list += string.punctuation\n",
    "#     stopped_tokens = [w.lower() for w in tokens if w not in stopwords_list]\n",
    "#     return stopped_tokens\n",
    "\n",
    "def process_tweet(text,as_lemmas=False,as_tokens=True):\n",
    "#     text=text.copy()\n",
    "    for x in find_urls(text):\n",
    "        text = text.replace(x,'')\n",
    "        \n",
    "    for x in find_retweets(text):\n",
    "        text = text.replace(x,'')    \n",
    "        \n",
    "    for x in find_hashtags(text):\n",
    "        text = text.replace(x,'')    \n",
    "\n",
    "    if as_lemmas:\n",
    "        from nltk.stem.wordnet import WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        text = lemmatizer.lemmatize(text)\n",
    "    \n",
    "    if as_tokens:\n",
    "        text = clean_text(text)\n",
    "    \n",
    "    if len(text)==0:\n",
    "        text=''\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.136893Z",
     "start_time": "2020-10-24T19:48:22.707Z"
    }
   },
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_processed_text(i=(0,len(corpus)-1)):\n",
    "    text_in = corpus[i]#.copy()\n",
    "    print(text_in)\n",
    "    text_out = process_tweet(text_in)\n",
    "    print(text_out)\n",
    "    text_out2 = process_tweet(text_in,as_lemmas=True)\n",
    "    print(text_out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.137802Z",
     "start_time": "2020-10-24T19:48:22.710Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Potential Tasks: Classify Android vs iPhone tweets (from period where Android tweets still exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.138768Z",
     "start_time": "2020-10-24T19:48:22.716Z"
    }
   },
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df['created_at'])\n",
    "df\n",
    "\n",
    "df = df.set_index('datetime').sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.139670Z",
     "start_time": "2020-10-24T19:48:22.719Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(process_tweet)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.140643Z",
     "start_time": "2020-10-24T19:48:22.724Z"
    }
   },
   "outputs": [],
   "source": [
    "android = df.groupby('source').get_group('Twitter for Android')\n",
    "android.index\n",
    "\n",
    "iphone = df.groupby('source').get_group('Twitter for iPhone').loc[:android.index[-1]]\n",
    "iphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.141584Z",
     "start_time": "2020-10-24T19:48:22.727Z"
    }
   },
   "outputs": [],
   "source": [
    "len(android), len(iphone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.142480Z",
     "start_time": "2020-10-24T19:48:22.730Z"
    }
   },
   "outputs": [],
   "source": [
    "df_corpus = pd.concat([iphone,android],axis=0)\n",
    "df_corpus['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Count vectorization\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "    -  Used for multiple texts\n",
    "    \n",
    "    \n",
    "**_Term Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ \\text{Term Frequency}(t) = \\frac{\\text{number of times it appears in a document}} {\\text{total number of terms in the document}} $$ \n",
    "\n",
    "**_Inverse Document Frequency_** is calculated with the following formula:\n",
    "\n",
    "$$ IDF(t) = log_e(\\frac{\\text{Total Number of Documents}}{\\text{Number of Documents with it in it}})$$\n",
    "\n",
    "The **_TF-IDF_** value for a given word in a given document is just found by multiplying the two!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions/Topics \n",
    "- Next time: vectorization\n",
    "- Vs Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.143463Z",
     "start_time": "2020-10-24T19:48:22.737Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-24T19:48:40.144463Z",
     "start_time": "2020-10-24T19:48:22.740Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.fit_transform(df_corpus['clean_text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "232.713px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
